\chapter{Introduction}

\section{Objectives of the thesis}

    \setcounter{page}{1}
    \vspace*{0.5cm}

    Proteins are large macromolecules in the form of chains of building blocks called amino acid residues,
    linked by peptide bonds. There are 20 common amino acid types, but certain proteins may contain 2
    additional amino acid types, namely pyrrolysine and selenocystein. % TODO: check
    Proteins are responsible for a wide range of functions within living organisms, including
    enzyme catalysis, transport of molecules, DNA replication, DNA repair, DNA transcription or cell signaling.
    Over 5000 types of biochemical reactions have been shown as being catalyzed by enzymes~\cite{schomburg2012brenda},
    which are mostly proteins. The number a ligands a protein can bind, namely the enzymatic specificity,
    can be determined by the structure of the protein itself~\cite{pi2004determination}.
    The region of a protein binding a substrate and containing
    the residues involved in the catalytic process is called the active site.
    Enzyme structure is thus of great importance since enzymatic specificity is crucial in novel drug discovery:
    molecules present in tested drugs are expected to have a structure with as large as possible specificity
    in order to avoid unwanted effects on the patient.

    According to Anfinsen's dogma, the three-dimensional structure of a protein is uniquely
    determined by its underlying amino acid sequence,
    at least when observed in protein's native environment. When moved from an unfavourable environment (where proper folding conditions are not met)
    to a solvent at neutral pH, a random coil (a sequence of amino acid residues
    oriented in random directions) will evolve towards the three-dimensional structure that minimizes Gibbs free energy.
    This process is called protein folding and has, however, a few exceptions.

    Protein structure is organized hierarchically: primary structure, secondary structure, tertiary structure
    and quaternary structure. Primary structure refers to the chemical composition of the protein, hence the sequence of amino acids present in it.
    Secondary structure indicates the presence of structures that are local to the amino acids themselves:
    these structures can generally be $\alpha$-helices
    or $\beta$-sheets. Tertiary structure contains information about the three-dimensional structure of the protein and results from interactions
    between side chains of some pairs of amino acids, such as hydrogen bonds, ionic bonds or disulfide bridges.
    Quaternary structure is specific to proteins having multiple polypeptide chains and describes the structure due to intermolecular interactions between
    these chains. PCP helps predicting the tertiary structure since three-dimensional models can be reconstructed from protein contact maps (PCM).
    Also, PCM is a more simplistic and robust description of a protein's geometry because it is invariant to rotations and translations.
    This simplification helps making deep learning methods perform well on structure prediction.

    Assuming the protein backbone has no structural restriction and is composed of $n$ residues holded together by $n-1$ peptide bonds,
    then the protein has $2(n-1)$ bond angles that can be each in three different stable states. Therefore, there are at most
    $3^{2(n-1)}$ possible configurations, and it would take the age of the universe to find the correct folding by enumeration.
    There is strong evidence that protein folding is a NP-hard problem~\cite{hart1997robust}.
    However, in practice small proteins are able to fold into a stable conformation in a fraction of a millisecond.
    This observation is known as the Levinthal's paradox. There has been a long standing perspective that protein folding
    is guided by heuristics composed of local interactions~\cite{levinthal1969fold}. Heuristic folding leads to misfolded proteins
    that can potentially cause genetic diseases.
    Luckily, some proteins are assisted by molecular chaperones during their folding process~\cite{ellis1991molecular}
    to attain their functional conformation. It must be noted that Anfinsen's observations of polypeptide chains refolding
    spontaneously in an aqueous medium have been made in the framework of in vitro studies:
    they do not take into account protein-protein interations and thus cannot generalize the self-assembly process well.

    Computational methods are important in structural biology,
    as they help in assigning biochemical or biological functions to proteins.
    Indeed, the three-dimensional structure of a protein is more conserved than the
    underlying amino acid sequence across evolution.
    Prompted by this knowledge, similar functions
    can be assigned to proteins with low structural dissimilarity.
    Precisely identifying the role played by each protein in an organism is the first step
    towards understanding complex body mechanisms like muscle contraction, digestion or perceiving light.
    Also, determining the static structure of proteins help in detecting misfolded proteins
    which are possibly involved in diseases like Parkinson's or
    Alzheimer's, but also in diagnosing those diseases~\cite{forloni2002protein}.
    Finally, solving the protein folding (structure prediction) problem will enable
    to do better protein design, for example to engineer enzymes like PETase
    so they have faster plastic-degrading capabilities.

    Protein Contact Prediction (PCP) can help determining the three-dimensional structure of
    proteins by limiting the search space to certain conformations
    constrained by predicted contact maps: this methodology is called contact-assisted protein folding.
    The problem of predicting the structure of a protein can be reduced to PCP because the latter is a much simpler problem,
    and only a few correctly predicted contacts are sufficient to reconstruct the whole structure~\cite{kim2014one}.
    There are pipelines designed to predict the structure and then the function of a newly
    observed protein, such as RaptorX server~\cite{peng2011raptorx}.

    Most PCP methods can be roughly divided into two categories:
    the ones based on Evolutionary Coupling Analysis (ECA) and the ones that infer contacts using
    supervised machine learning. In the former case, amino acid pairwise mutations are statistically modelled and the underlying model's parameters
    are generally optimized through log-likelihood maximization. In the second case, deep neural architectures are used to
    refine predictions made by low-level predictors such as ECA, in order to generate high-quality contact maps.

    Ultimately, PCP should help making \textit{ab initio} structure prediction.
    However, most recent methods rely on a whole raft of alignment and prediction tools.
    Given a protein encoded in FASTA format, ECA is only possible using a Multiple Sequence Alignment (MSA)
    of this target protein against homologuous proteins. These homologuous proteins come from the same protein family
    as the target protein, and therefore the most suitable family must be found.
    This can be done by matching the target sequence to a Hidden Markov Model (HMM) profile representing a family
    like in Pfam database~\cite{Pfam}. Once the homologuous sequences have been retrieved, they have to be aligned to
    the target sequence using an MSA tool like HHblits or HMMER. In the next step, evolutionary couplings are extracted from
    the MSA using an ECA predictor like PSICOV~\cite{doi:10.1093/bioinformatics/btr638} or plmDCA~\cite{EKEBERG2014341}.
    Eventually, predictions are gathered and refined using a deep neural architecture, necessitating the use
    of a deep learning framework. These successive layers of dependencies are not making PCP a straightforward process.
    Therefore, it seems to be a natural choice to set as an objective for this thesis the development of a predictor with
    minimal requirements and performance close to state-of-the-art techniques
    \cite{RaptorX, DeepContact, doi:10.1093/bioinformatics/bty341, Michel383133, DeepMind}.

\section{Structure of the thesis}

    Let's describe the global view of the thesis itself.
    Firstly, common state-of-the-art ECA techniques will be described, such as Direct Coupling Analysis (DCA)
    and Pseudo-Inverse Covariance matrices (PSICOV) (both are statistical methods based on graphical models),
    as well as the basics of deep learning and backpropagation algorithm.
    In order to gain a deeper insight on best-performing methods, more details will be given for
    some specific deep learning methods, such as their architecture, input features and preprocessing.
    The generic architecture in use for this thesis will be detailed, as well as the hyper-parameter
    optimization procedure used for cross-validation.
    Finally, results will be presented in multiple sections:
    \begin{itemize}
        \item Since the proposed deep learning approach relies on DCA predictions as input features,
        the first step should demonstrate that deep learning is capable of refining
        contact maps by looking at complex visual patterns that cannot captured by linear models.
        \item It should be brought to light whether deep learning's performance is less sensitive to the effective
        number of homologous sequences than DCA methods. The notion of effective number of homologous sequences
        will be introduced in section \ref{meff}.
        \item Finally, a benchmark will be established to assess the performance of the proposed method
        in comparison with other supervised approaches, including state-of-the-art deep learning architectures.
    \end{itemize}

\section{Contributions}

    

\chapter{Materials and methods}

\section{Datasets}

  Five datasets have been used in the framework of this thesis:
  one for training the model, one for optimizing the hyper-parameters, and three for benchmarking.
  The training set is a set of 354 proteins including the 150 families reported in the original PSICOV
  paper~\cite{doi:10.1093/bioinformatics/btr638}, plus a subset of the first benchmark set used by
  Michel et al. to evaluate PConsC3~\cite{Skwark079673}.
  The validation set is composed of the 30 protein families from the validation set of PConsC3,
  which itself is a subset of the test set of PConsC2~\cite{10.1371/journal.pcbi.1003889}.

  Three test sets have also been considered in order to make a direct comparison with the state-of-the-art RaptorX-Contact predictor.
  The first test set embodies the 105 protein domains from the CASP11 experiment, the second test set 76 proteins from the CAMEO
  project, and the fourth test set 398 membrane proteins.

  \subsection{Homology reduction}

    A straightforward method for reducing homology between two set of proteins is to
    remove proteins that have a sequence identity rate above a given threshold.
    As a rule of thumb, this threshold is usually set to 40\%.
    Identity rates were computed by running Needleman-Wunsch algorithm on each
    pair of proteins coming from two different datasets. Score matrix was set
    such that exact matches give a score of 1 and any mismatch gives a penalty of -1.
    Indeed this approach promotes global alignments with maximum identity rates.

    \begin{table}[H]
      \centering
      \begin{tabular}{|l|c|c|c|}
        \hline
        Identity rate & Minimum & Average & Maximum \\
        \hline
        \hline
        Training set - CASP11 & 5.4 & 22.4 & 32.5 \\
        PSICOV150 - CASP11 & 7.3 & 22.9 & 39.7 \\
        PSICOV150 - CAMEO & 6.2 & 21.7 & 34.4 \\
        Training set - CAMEO & 4.0 & 21.3 & 32.5 \\
        PSICOV150 - Membrane & 7.6 & 22.2 & 74.0 \\
        \hline
      \end{tabular}
      \captionof{table}{Identity rates between training and benchmark sets, expressed as percentages.}
      \label{identityrates}
    \end{table}

    Similarity rates are more informative than identity rates because amino acids
    are very likely to mutate across evolution towards amino acids that share
    similar physico-chemical properties, and such measures take these mutations
    into account. However, ECOD H-classes~\cite{10.1371/journal.pcbi.1003926}
    were used instead of similarity rates because they can potentially give more
    evidence on whether two sequences are evolutionary related.
    Therefore, proteins belonging to different datasets and sharing the same
    ECOD H-class have been rejected.

    \todo{Computation of identity rates, Common ECOD H-classes}

    \begin{figure}[H]
      \begin{center}
        \includegraphics[width=\textwidth, keepaspectratio]{imgs/datasets.png}
         \caption{Homology reduction between the different datasets}
        \label{homology_reduction}
      \end{center}
    \end{figure}

  \subsection{PSICOV Dataset}

    The PSICOV~\cite{doi:10.1093/bioinformatics/btr638} dataset is composed of 150 families
    and associated multiple sequence alignments
    taken from the Pfam database, each containing more than 1000 homologous sequences
    and a target sequence with high-resolution ($\le$ 1.9 \AA{}) X-ray crystallographic structure.
    Each target sequence contains exactly one copy of the Pfam domain, has a length lower than
    275 and greater than 50 residues. The number of unique sequences in each multiple sequence
    alignment strongly varies from one family to another.
    AraC-like ligand binding domain (implied in DNA-binding transcription and
    sequence-specific DNA binding) accounts for 511 unique sequences, compared to 74 836 sequences
    for the response regulator receiver domain.

  \subsection{CASP11}

    CASP10~\cite{doi:10.1002/prot.24452}
    CASP11~\cite{doi:10.1002/prot.25064}


    \todo{}

  \subsection{CAMEO}

    CAMEO~\cite{haas2013protein}

    \todo{}

  \subsection{Membrane proteins}

    \todo{}

  \subsection{Feature extraction}

    \todo{Profile HMMs: \cite{eddy1998profile}}
    MSAs have been created using HHblits~\cite{HHblits} (version as of the date of 26th February 2016) on the Uniprot20 database
    with an e-value of 1. Parameters have been set in such a way that all sequences in each of the database MSAs are aligned.
    The obtained MSAs have been used as input to all other predictors and intermediate predictors, allowing for easier comparability.

    All protein sequences, structures, MSAs and intermediate predictions used in this thesis come from the datasets that were publicly
    available at the address \url{http://pconsc3.bioinfo.se/pred/download/} as on the date of 28th December 2018.
    Information available in these datasets is the following:

    \todo{PDB parser to obtain distance and contact maps}

    \begin{itemize}
      \item Protein sequence in FASTA format
      \item \index{MSA} obtained using HHblits on the corresponding protein family
      \item Atom 3D coordinates
      \item PhyCMAP~\cite{PhyCMap} intermediate predictions
      \item plmDCA~\cite{EKEBERG2014341} intermediate predictions
      \item GaussDCA~\cite{10.1371/journal.pone.0092721} intermediate predictions
      \item Predictions made by PConsC3~\cite{Skwark079673} at each layer of the model
      \item CCMPred~\cite{CCMPred} predictions (only available in the 4 test sets)
      \item EVFold~\cite{Sheridan021022} predictions (only available in the 4 test sets)
      \item PSICOV~\cite{doi:10.1093/bioinformatics/btr638} predictions (only available in the 4 test sets)
      \item MetaPSICOV~\cite{MetaPSICOV} predictions (only available in the 4 test sets)
    \end{itemize}

    \todo{Took alignments from PConsC2 and PConsC3 -> model not influenced by the new releases of alignments tools}
    \todo{What about the protein structures?}

    \todo{Oversampling negative class: \cite{markowski2016oversampling}}

\section{Summary of input features}

  As described in section \ref{inputfeatures}, input features can be split into three categories:
  global, 1-dimensional and 2-dimensional features.
  The proposed model takes as input the protein length, the effective number of sequences
  in the corresponding MSA, position-specific statistics,
  residue pair-specific statistics, and predictions made by PSICOV, plmDCA and GaussDCA.
  Additionaly to these features, secondary structure, solvent accessibility and
  region disorder are predicted by RaptorX-property server and added to the rest of
  1-dimensional features.
  \todo{cite RaptorX-property}

  \begin{table}[H]
    \centering
    \begin{tabular}{|l|l|c|}
      \hline
      Category & Feature name & Dimensionality \\
      \hline
      \hline
      Global & Protein length $L$ & scalar \\
             & Effective number of sequences $M_{eff}$ & scalar \\
      \hline
      1-dimensional & One-hot-encoded sequence & $L \times \naatypes$ \\
                    & Self-information & $L \times \naatypes$ \\
                    & Partial entropy & $L \times 2 \cdot \naatypes$ \\
                    & Predicted secondary structure & $L \times 3$ \\
                    & Solvent accessibility & $L \times 3$ \\
                    & Region disorder & $L$ \\
      \hline
      2-dimensional & Mutual information & $L \times L$ \\
                    & Normalized mutual information & $L \times L$ \\
                    & Cross-entropy & $L \times L$ \\
                    & PSICOV predictions & $L \times L$ \\
                    & GaussDCA predictions & $L \times L$ \\
                    & plmDCA predictions & $L \times L$ \\
      \hline
    \end{tabular}
    \captionof{table}{Input features of the proposed model.
      Global features are scalar values, whereas dimensional
      features are presented in the form of matrices of given shape.}
    \label{hyperparams}
  \end{table}

\section{Proposed architecture}

  Proposed predictor, as illustrated in figure~\ref{architecture}, takes ideas from both PConsC4~\cite{Michel383133} and 
  RaptorX-Contact~\cite{RaptorX} architectures (see section \ref{dlpredictors}).
  Global features, one-dimensional and two-dimensional features are being fed
  as input to the global, 1D and 2D modules, respectively.
  \begin{itemize}
    \item \textbf{Global module} is made of a succession of multiple fully-connected layers.
        Its output is repeated in order to match the dimensionality of the 1D module's input.
    \item \textbf{1D module} is a one-dimensional residual network.
        Each layer is composed of 1D convolution, 1D batch normalization and activation function.
        Its input is a concatenation of one-dimensional features and global module's output.
        A Kronecker product is applied between the module's output and itself
        in order to match the dimensionality of the 2D module's input.
    \item \textbf{2D module} is a two-dimensional residual network.
        Each layer is composed of 2D convolution, 2D batch normalization and activation function.
        Its input consists of a concatenation of the 2D features and the output of the 1D module.
  \end{itemize}
  The output of the 2D module is the output of the whole model. Contrary to RaptorX-Contact,
  it does not represent a single contact map but 5 contact maps predicted at contact thresholds
  $6$, $7.5$, $8$, $8.5$ and $10$ \AA{}, like in PConsC4.
  In contrast to the other two, proposed architecture is fully-convolutional and handles variable-length
  proteins even with a batch size greater than one.

  \begin{figure}[H]
    \begin{center}
      \includegraphics[width=\textwidth, keepaspectratio]{imgs/architecture.jpg}
       \caption{Proposed architecture of the deep convolutional neural network for semantic segmentation}
      \label{architecture}
    \end{center}
  \end{figure}

\section{Evaluation}

  \subsection{Contact map evaluation}

    Protein contact maps are imbalanced by nature: they contain very few residue contacts compared to their number of residue pairs.
    $L$ being the number of residues in a protein, the number of residue contacts increases linearly with $L$ while the number of residue pairs
    increases quadratically~\cite{OLMEA1997S25}. This is important because one can evaluate a model only on the $L$ (or even less than $L$) predicted
    residue contacts the model is the most confident about. Such evaluation metric is called \textit{best-L/k PPV (Positive Predictive Value)}
    and can be formulated as follows:

    \begin{equation}
      \text{Best-L/k PPV} = \frac{\sum_{\underset {i-j \geq 6}{(i, j) \in B(L/k)}} C_{i, j}}{L/k}
    \end{equation}

    where $C_{i, j} \in \{0, 1\} \ \forall i, j, i-j \ge 6$ are boolean values indicating a predicted contact.
    $B(L/k)$ is the set of $L/k$ residue pairs with most confident (highest) predicted probabilities.
    Contacts under a residue distance of 6 amino acids are not considered during evaludation phase even though they are used during
    training phase.

    Best-L/k PPV can also be split into three separated metrics: short-range, medium-range and long-range contacts.
    These three types of contacts can be defined by the residue separations used by Skwark et al.~\cite{10.1371/journal.pcbi.1003889}:

      \begin{itemize}
        \item Short-range contacts: 6 - 12 residue separation
        \item Medium-range contacts: 12 - 24 residue separation
        \item Long-range contacts: 24+ residue separation
      \end{itemize}

    \subsection{3D model evaluation}

      \todo{TM-score and RMSD}

\section{Hyper-parameter optimization}

  In order to ensure the best hyper-parameters are selected for the model that will be evaluated
  on the benchmark sets, the Hyperopt Python library~\cite{Bergstra_2015} has been used to explore
  the hyper-parameter space and fine-tune the model on the validation set.
  Training and evaluating a deep neural network is very costly and, as a matter of fact,
  each trial point in the hyper-parameter space should be carefully selected. Techniques based
  on grid search do not suit the problem because they are uninformed methods.

  Hyperopt provides an informed search method called Tree-structured Parzen Estimators (TPE)~\cite{bergstra2011algorithms}.
  In Bayesian hyper-optimization, the posterior probability $P(\alpha \vert \mathcal L)$ is defined as a function
  of the hyper-parameter vector $\alpha$ and the loss $\mathcal L$. Contrary to techniques based on Gaussian processes
  that approximates $P(\mathcal L \vert \alpha)$ directly, TPE models both posterior $P(\alpha \vert \mathcal L)$ and $P(L)$.
  The prior is iteratively replaced with non-parametric densities based on generated points $\{ \alpha_1, \alpha_2, \dotsc \}$.
  It this search, TPE is an informed search strategy that refines its prior as new points are observed in the
  hyper-parameter space. The "tree structure" is due to the way the posterior is computed.

  Let $f$ be the prediction function of the model (see section \ref{backpropagation} about backpropagation algorithm),
  and let $f(\alpha) \triangleq \text{argmin}_{f(\Theta, \alpha)} \ell \big(f(\Theta, \alpha)\big)$ be the prediction function  % Formalism: meh
  of a trained model that minimizes a given loss function $\ell$ w.r.t. a fixed hyper-parameter vector $\alpha$.
  Let $l(\alpha)$ be the non-parametric density function defined as a mixture of density functions centered each on an
  observation $\{ \alpha^{i} \}$ for which $\mathcal L = c(f(\alpha^{i}))$ is below the threshold $\mathcal L^*$.
  Density function $g(\alpha)$ is defined analogously as a mixture if density functions centered each on one of the
  remaining observations.
  As described in the following equation, the density function to be used to approximate the posterior is
  determined according to whether the threshold $\mathcal L^*$ has been exceeded.

  \begin{equation}
    P(\alpha \vert \mathcal L) =
      \begin{cases}
        l(\alpha) &  \text{if} \, \mathcal L < \mathcal L^* \\
        g(\alpha) &  \text{otherwise}
      \end{cases}
  \end{equation}

  The threshold $\mathcal L^*$ is set as a quantile of the observed values of $\mathcal L$.
  The value to be optimized in TPE is the Expected Improvement (EI), measured as an integral
  of loss improvements weighted by the posterior itself.
  After applying Bayes formule to the posterior, calculus of EI becomes:

  \begin{equation}
    EI_{\mathcal L^*}(\alpha) = \int_{-\infty}^{\mathcal L^*} (\mathcal L^* - \mathcal L) P(\mathcal L \vert \alpha) d\mathcal L
    = \int_{-\infty}^{\mathcal L^*} (\mathcal L^* - \mathcal L) \frac{P(\alpha \vert \mathcal L) P(\mathcal L)}{P(\alpha)} d\mathcal L
  \end{equation}

  In the framework of Adaptive Parzen Estimators, to each hyper-parameter is assigned a prior and a density function,
  and the estimator is built as a weighted mixture of them.
  For example, a continuous variable can be assigned:
  \begin{itemize}
    \item A uniform prior with lower bound $a$ and upper bound $b$.
    \item A function defined as a mixture of Gaussian distributions, each centered on a point of the hyper-parameter
    space. The standard deviation of a particular distribution can be set as the maximum between distances to the left and right
    neighbors.
  \end{itemize}

  The density function is either $l(\alpha)$ or $g(\alpha)$ depending on whether the loss function associated to current
  point is below the threshold or not.

  \begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|}
      \hline
      Module & Hyper-parameter & Set of values \\
      \hline
      \hline
      General & Batch size & $\{ 1, 2, 4, 8, 16, 32 \}$ \\
              & Batch normalization & $\{ \top, \bot \}$ \\
              & Track running state & $\{ \top, \bot \}$ \\
              & Learning rate & \text{TODO} \\
              & L2 penalty & \text{TODO} \\
              & Parameter optimization & $\{ \text{ADADELTA}, \text{Adagrad}, \text{Adam} \}$ \\
              & Activation function & $\{ \text{ReLU}, \text{ELU}, \text{LeakyReLU}, \text{Tanh} \}$ \\
              & Use global modules & $\{ \top, \bot \}$ \\
      \hline
      Global module & Depth & $\{ 2, 3, 4, 5, 6, 7, 8, 9, 10 \}$ \\
      \hline
      1-dimensional module & Depth & $\{ 2, 3, 4, 5, 6, 7, 8, 9, 10 \}$ \\
                           & Filter size & $\{ 3, 5, 7 \}$ \\
                           & Number of filters & $\{ 8, 16, 32, 64, 128 \}$ \\
      \hline
      2-dimensional module & Depth & $\{ 2, 3, 4, 5, 6, 7, 8, 9, 10 \}$ \\
                           & Filter size & $\{ 3, 5, 7 \}$ \\
                           & Number of filters & $\{ 8, 16, 32, 64, 128 \}$ \\
      \hline
    \end{tabular}
    \captionof{table}{Hyper-parameter space for the proposed architecture.}
    \label{hyperparams}
  \end{table}

\section{Implementation}

  \subsection{Availability}

    Source code is available at: \,
    \href{https://github.com/AntoinePassemiers/Wynona}{https://github.com/AntoinePassemiers/Wynona}.

  \subsection{Deep learning framework}

    Methods and results presented in this thesis have been both implemented and produced in
    Python. The neural architecture has been built on top of PyTorch,
    which is an open source deep learning framework based on Torch~\cite{torch}.

    PyTorch does not natively handle arbitrary-sized inputs, for example fully-convolutional
    neural networks cannot accept images with variable height/width.
    For this aim, it is necessary to add a layer of abstraction so the neural models are
    able to process \textit{virtual batches} of inputs. Let's define a virtual batch as the number
    of samples a model has to process between each parameter update.
    A forward pass on a virtual batch then consists in constructing one computational graph
    per sample and backpropagate the gradients through each one of them separately.
    Once all the gradients have been computed, they are collected and averaged over the sample
    dimension. Pytorch allows to explicitly call the forward pass, backward pass and update
    procedures when needed, which eases the implementation of virtual batch processing.

    Model general architecture is fully-convolutional, forcing us to use only deep learning
    functionalities that are invariant to individual input sizes. These are:

    \begin{itemize}
      \item Element-wise operations like activation functions: ReLU, ELU, Sigmoid, etc.
      \item Dropout, which preserves the dimensionality of its inputs.
      \item Convolution, because convolutional filters have a dimensionality that is
      invariant to the input size (see section \ref{convlayers}).
      \item Batch normalization (see section \ref{batchnorm}).
      \item Many non-neural transformations like arithmetic operations, Einstein
      summation, Kronecker product, etc.
    \end{itemize}

  \subsection{Feature extraction}

    Many features discussed in the present document rely on amino acid counts.
    Despite the fact that counting algorithms such as histograms are embarassingly parallel
    and naturally suitable for multiprocessing, they cannot be efficiently vectorized.
    For this specific reason, the scientific computing library NumPy (which has been extensively
    used during the experiments) is not sufficient to extract this type of features in reasonable
    time. Instead, C extensions have been created using the Cython compiler~\cite{behnel2010cython}.

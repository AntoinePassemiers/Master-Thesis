\section{Deep learning and Protein Contact Prediction}

    \subsection{Fully-convolutional networks}

        As a reminder, a fully-convolutional network is a neural network capable
        of handling variable sized input. Therefore, the dynamic input dimensions
        cannot be processed by a fully-connected neural layer.
        An example of such an architecture is DeepCov~\cite{doi:10.1093/bioinformatics/bty341},
        a deep neural network composed only of 2D convolutional layers and an additional
        maxout input layer. A maxout layer is made of a convolutional layer, followed
        by a max-pooling operation. Dimensions of intermediate feature maps are preserved
        by convolutions using a stride of one and a "same-padding". In most deep learning
        libraries, padding of type "same" refers to the padding applied by convolutional layers
        to maintain the sizes of spatial (convoluted) dimensions.
        In DeepCov, the only features are the covariance matrices as computed
        in equation \ref{covariance}. Couplings matrices predicted by DCA methods can be fed as
        input to the model instead of covariance matrices, as shown in plmConv study~\cite{golkov2016protein}.

        The approach described in the DNCON2 paper~\cite{doi:10.1093/bioinformatics/bty341}
        also implements convolutional layers with dynamic spatial dimensions.
        Additionally to that, the fuzziness of residue contact definition is handled
        by training one fully-convolutional neural network per contact threshold.
        More specifically, five networks are trained to output contact maps at 6, 7.5, 8, 8.5 and 10 \AA{}
        thresholds, respectively. These five are stacked on top of a sixth network in charge
        of refining and combining the predictions into a final contact map at a threshold of 8 \AA{}.

    \subsection{Residual Networks (ResNets)}

        As described in section \ref{backpropagation} about backpropagation,
        the loss gradient with respect to the parameters of a specific layer
        is computed as the product of many other mathematical entities
        (vectors, scalars, matrices, etc.), and the number of factors in such
        a product grows linearly with the number of operations applied after
        current layer. When this number is too large, some layers may be
        updated with numerically unstable gradients.

        A widely used solution is to add
        residual connections~\cite{DBLP:journals/corr/HeZRS15}
        to the architecture. The latter can thus no longer be viewed as
        a regular composition of functions and must take into account
        the residual mapping at the end of each residual block.
        The output $Y^{(r)}$ of residual block $k$ should now
        be formalized with a more general form:

        \begin{equation}
            Y^{(r)} = f\big(X^{(r)}, \{W^{(p)}\}_p\big)
        \end{equation}

        where $W^{(p)}$ are the weights of a layer $p$ in block $k$.
        Residual mappings can be implemented in several ways and figure \ref{resnet}
        illustrates one of them.

        \begin{figure}[H]
            \begin{center}
                \includegraphics[width=\textwidth, keepaspectratio]{imgs/resnet.png}
                \caption{Illustration of a residual connection in a convolution
                neural network. For the element-wise sum to work, the input and output
                of the residual block are required to be of the same shape.}
                \label{resnet}
            \end{center}
        \end{figure}

    \subsection{Deep fully-convolutional residual networks}

        Most successful methods rely on very deep architectures with residual mappings.
        One-dimensional features are processed by one-dimensional residual network
        before being concatenated with two-dimensional features. The resulting tensor
        is the input of a two-dimensional residual network.
        Examples of such approach is DeepContact~\cite{DeepContact} and the state-of-the-art
        RaptorX-contact predictor~\cite{DeepContact}. Both methods rely on CCMPred contact
        prediction, solvent accessibility and secondary structure prediction.
        Additionally to CCMPred, DeepContact incorporates EVFold predictions together
        with the rest of the two-dimensional features. Also, global features (e.g. the number
        of effective sequences) are tiled and concatenated with other features: this does
        not impact the fully-convolutional property of DeepContact because global features
        are invariant to the protein length.
        The largest difference in the two methods lies in the depth of the networks:
        9 convolutional layers for DeepContact and 60-70 layers for RaptorX-contact.
        It must be noted that RaptorX-contact architecture is not fully-convolutional
        since zero-padding is applied to feature maps when more than one protein
        is processed in a batch.

    \subsection{U-net architecture}

        In PconsC4~\cite{Michel383133}, the model is partly built on top of the U-net
        architecture~\cite{DBLP:journals/corr/RonnebergerFB15}, and trained on a set
        of 2891 proteins retrieved from PDB. Features are divided in one-dimensional
        inputs including one-hot-encoded amino acids, self-information and
        partial entropies, and two-dimensional inputs including mutual information,
        normalized mutual information and cross-entropy.
        For both mutual information and normalized mutual information,
        average product correction is applied.
        One-dimensional features are convoluted through one-dimensional residual networks~\index{residual network},
        concatenated and finally reshaped to two-dimensional maps with an outer product.
        After reshaping, the intermediate feature maps are concatenated with the
        two-dimensional features and the whole is fed as input to the U-net architecture.

        Models designed for semantic segmentation problems (including PCP) have been shown
        to gain significant performance when additional connections are allowed between
        layers close to the network's input and layers close to the network's output~\cite{huang2017densely}.
        U-net architectures develop this idea: they have a somehow symmetric structure made of two sub-networks
        that compress and decompress the information, respectively.
        The first sub-network successively alternates between convolutional transformations
        and max-pooling, while the second sub-network alternates between convolutional
        transformations and transposed convolutions (also called upsampling).
        Similarly to autoencoders, the features maps
        processed the middle layers or of much smaller dimensionality than input or output tensors.
        To prevent the whole architecture from forgetting contextual information due to
        compression, shortcut connections are added between tensors of identical dimensionality.
        Contrary to residual networks, these shortcut connections are implemented by
        concatenation instead of addition. Due to the max-pooling and upsampling operations,
        the width and height of input images should preferrably be a power of two.
        Because PconsC4 model outputs entire contact maps, and because proteins are
        of variable length by nature, input feature maps of a particular protein are zero-padded
        to the smallest power of two that is larger than the protein length.
        Finally, PconsC4 archicture does not only predict a contact map but predicts multiple contact maps
        at different distance thresholds.

    \subsection{Dense networks (DenseNets)}

        Dense networks extend the idea of residual networks by allowing residual mappings
        between all layers, resulting in an ergodic topology. In DenseNets~\cite{huang2017densely},
        shortcut connections are implemented with a concatenation operation instead of addition.
        Nevertheless, spatial dimensions are still constrained to be identical between the source and the destination
        of a shortcut connection. Figure \ref{densenet} illustrates a dense block composed of
        2 convolution layers: it is shown how connections are allowed between all entry points and layer outputs.

        \begin{figure}[H]
            \begin{center}
                \includegraphics[width=0.8\textwidth, keepaspectratio]{imgs/densenet.png}
                \caption{Illustration of dense blocks from the DenseNet paper~\cite{huang2017densely}.
                All layer entry points and outputs belonging to a dense block are connected
                by residual mappings.}
                \label{densenet}
            \end{center}
        \end{figure}

        Each unit of a dense block is the composition of batch normalization, activation and
        convolution operations.

    \subsection{TiramiProt}

        TiramiProt is based on the Tiramisu architecture~\cite{TsardakasRenhuldt1228846}
        which tries to combine the ideas of both U-net and DenseNet.
        Like U-net architectures, Tiramisu applies max-pooling for reducing dimensionality
        and upsampling to restore the input dimensionality. Each dense block output from
        the downsampling part of the network is, by symmetry, connected to the entry point
        of the corresponding dense block in the upsampling part of the network.
        As in U-net, these shortcut connections are implemented by feature map concatenation.
        Training set and features are the same as in the Pconsc4 study.

    \subsection{DeepConPred2}

        DeepConPred2~\cite{DeepConPred2} is based on a problem-specific architecture made
        of three modules. First module consists of a Deep Belief Network (DBN) that predicts
        contacts between secondary structures from CCMPred coevolutionary information.
        A DBN is a graphical model implemented as
        a stack of unsupervised building blocks such as autoencoders or
        restricted Boltzmann machines.
        The output of the first module, together with solvent accessibility and secondary
        structure prediction, is fed as input to each of the DBNs composing
        the second module: one for short-range contacts, one for medium-range contacts
        and three for long-range contacts (taken from previous study~\cite{xiong2017deep}).
        Each of these components are used to predict actual residue contacts.
        Then, each DBN output is fed as input to one of the ResNets of the third module.
        Each ResNet has 50-80 convolutional layers with Leaky-ReLU activation functions.

    \subsection{Properties of DL approaches}

        PCP methods that have just been described are summarized in table \ref{overview}
        according to some indicators such as training set size, fully-convolutional and fuzzy
        aspects of the approach and depth of the architecture. RaptorX-contact has not been
        counted as a fully-convolutional approach since zero-padding is used for batches of
        more than one protein. Depth of the network has been measured as the number of non-linearities,
        that is to say the number of layers or blocks preceding an activation function.
        Additionaly, the table shows which methods incorporate contacts defined at different distance
        thresholds.

        \begin{table}[H]
            \centering
            \resizebox{\textwidth}{!}{
            \begin{tabular}{lcccc}
                \hline
                 & Training set size & Fully-convolutional & \# non-linearities & Fuzzy \\
                \hline
                \hline
                DNCON2 & 1230 & - & 6 & $\top$ \\
                DeepCov & 6003 & $\top$ & 14 & $\bot$ \\
                plmConv & 231 & $\bot$ & 4 & $\bot$ \\
                DeepConPred2 & 3443 & - & 50-80 & $\bot$ \\
                DeepContact & - & $\top$ & 9 & $\bot$ \\
                PconsC4 & 2891 & $\bot$ & - & $\top$ \\
                TiramiProt & 2891 & $\bot$ & 16 & $\top$ \\
                RaptorX-Contact & $\sim 6000$ & $\bot$ & 60-70 & $\bot$ \\
                \hline
            \end{tabular}
            }
            \captionof{table}{Overview of different deep learning models for PCP:
            number of proteins in the training set, whether the architecture is
            fully-convolutional, how deep (measured in activation functions) the model is,
            and whether it learns from contact maps defined with multiple distance thresholds.}
            \label{overview}
        \end{table}

        In PconsC4 and TiramiProt, a single network predicts multiple contact maps at the same time, while
        DNCON2 uses multiple networks to each predict a contact map at a different threshold.

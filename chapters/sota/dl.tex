\section{Deep learning}

    \subsection{A definition of \textit{deep learning}}

        Behind the trendy words, it is quite difficult to find a consensus on the definition of \textit{deep learning}.
        According to many, the process of learning
        deeply can only be achieved by deep neural networks. A deep artificial neural network is composed of a set of parameters and a large stack
        (or graph) of mathematical operators designed to minimize an objective function.
        Each operation may be dependent on a subset of the network parameters.
        In most cases, the network can be described as a stack of operators.
        The objective function is then a composition of all mathematical operations. Such a network is usually trained using the
        backpropagation algorithm. The method consists in minimizing the objective function which is usually an average distance between what the network
        predicts on the basis of an input and what the human supervisor expects. More specifically, it is an iterative algorithm that evaluates the gradient of the
        objective at each iteration and performs one step in the direction of the steepest descent in the parameter space. The algorithm is expected to stop
        once a global minimum has been reached.

        Deep learning is also often viewed as the capacity of a machine to create a hierarchical modelling of the data.
        This implies that the model can transform the data to high-level feature maps.
        According to Yoshua Bengio and Yann LeCun, neural networks only exemplify the notion of deep
        architectures. They provided a sufficiently good basis for a definition:

        \begin{quotation}
            Deep architectures are compositions of many layers of adaptive non-linear components,
            in other words, they are cascades of parameterized non-linear modules that contain
            trainable parameters at all levels. Deep architectures allow the representation of wide
            families of functions in a more compact form than shallow architectures, because they
            can trade space for time (or breadth for depth) while making the time-space product
            smaller, as discussed below. The outputs of the intermediate layers are akin to intermediate
            results on the way to computing the final output. Features produced by the lower
            layers represent lower-level abstractions, that are combined to form high-level features
            at the next layer, representing higher-level abstractions~\cite{40d5d7fd62cb44ba934a8a75d4b2b076}.
        \end{quotation}

        This definition seems to be suitable for stacked models: one can design a cascade of decision tree modules since decision trees are able to construct
        non-linear decision boundaries. The problem rather lies in determining if there exists a natural way to make them extract high-level features from data.
        Apart from that, the backpropagation algorithm for neural networks is going to be introduced in more details.

    \subsection{The backpropagation algorithm} \label{backpropagation}

        A description of backpropagation has to be made in order to understand how modern deep learning works.
        Let's consider a feedforward neural network containing no cycle.
        Each of its layers can be viewed as a couple $(f_i(\theta_i, X), b_i(\theta_i, dX))$,  % Is dX standard notation? Does not seem adapted formally... dX is meant to be a function (differential)
        where $f_i$ is the forward pass function (prediction function)
        of layer $i$, $b_i$ is the backward pass function, $\theta_i$ is the set of parameters,
        and $X, Y$ are input tensors of shapes compatible with $f_i$ and
        $g_i$, respectively, and $dX$ is the signal tensor propagated from next layer to current layer.
        Let's make the assumption that convolutional layers are two-dimensional and that input instances are image-like data.
        Also, let's consider a particular case of neural network consisting of a stack of neural layers instead of a graph:
        a feedforward neural network.
        Also, let $b$ be the number of instances in the input tensor (more commonly referred to as the batch size),
        $w$ and $h$ respectively the width and height of the images,
        and $c$ the number of channels. Finally, let $n$ be the number of layers and $m$ be the number of output neurons in the network.
        Knowing this, we are able to express the output $Y \in \mathbb{R}^{b \times m}$ of the network as such:

        \begin{equation}
            Y = (\bigcirc_{i=1}^{n} f_{\theta_i})(X)
        \end{equation}

        where $X \in \mathbb{R}^{b \times w \times h \times c}$ and $f_{\theta_i}(X)$
        is a more convenient notation for $f_i(\theta_i, X)$. We observe that the prediction
        function of the network is basically a large composition of functions.

        Such model is designed to optimize a function reflecting its ability to accurately predict a target value or to abstractly represent the input
        data in a more general sense. Accordingly, let's introduce a generic loss function $L(Y): \mathbb{R}^{b \times m} \rightarrow \mathbb{R}$ that measures
        the model's inability to fulfill the given task. Again, the loss function can be rewritten as a composition of the layer forward passes and the loss  % Check formulation: loss is not the composition of the prediction function and the loss.
        function, and the objective is to find the set of parameters that minimizes the loss function:

        \begin{equation}
            \hat{\theta} = \argmin_{\theta \in \Theta} \ell((\bigcirc_{i=1}^{n} f_{i, \theta_i})(X))  % Why use $\ell$ if you defined $L$ above?
        \end{equation}

        where $\Theta$ is the set of all possible values for the parameter set $\theta = (\theta_1, \ldots, \theta_n)$.
        Since we are considering a loss function, the latter must be minimized.
        The generic task of minimizing a scalar continuous function can be achieved
        using numerous continuous optimization techniques  among gradient descent algorithms~\cite{DBLP:journals/corr/Ruder16}
        or quasi-Newton methods~\cite{LBFGS}. In practice, gradient descent approaches require more iterations to converge to a satisfying solution,  % L-BFGS <3
        but are much easier to implement. Also, contrary to quasi-Newton methods, they don't require to implicitly compute the hessian matrix of the loss
        function according to the network parameters, which makes them less computation-intensive. Let's consider the optimization of the loss function
        in the gradient descent framework. The loss function is minimized by moving in the parameter space in the direction of the loss gradient, with
        a step proportional to a parameter either determined empirically or adjusted during optimization phase, called learning rate.
        Luckily, since we are regarding our neural network as a stack of layers (viewed as nested functions), the gradient computation can be decomposed  % nested function? Weird terminology: prefer composition of functions.
        using the chain rule:

        \begin{equation} \label{eq:backprop}  % Formalism? 'prime' notation with vectorial functions... Maybe write expressions for one partial derivative
            (f \circ g)'(X) = \nabla f(g(X)) \cdot g'(X)
        \end{equation}

        Knowing this, the gradient of $loss(\theta)$ according to the parameters $\theta_j$ of layer $j$ (for any layer $j$ with learnable parameters),  % find notation for loss(\theta). Can't use \ell which is used for loss on prediction, not on the parameter. Plus $m$ is already used for the number of output neurons so use another letter
        can be decomposed as the following product:

        \begin{equation} \label{eq:loss}  % Again: use of \ell for function of parameters is unclear
            \nabla \ \ell(\theta_m) = \prod_{k=1}^m f_{\theta_k}'\left((\bigcirc_{i=1}^{k} f_{\theta_i})(X)\right) \cdot L'\left((\bigcirc_{j=1}^{n} f_{\theta_j})(X)\right)
        \end{equation}  % Using $i$ as upper bound of composition is very weird.

        Each factor $k$ of the product can be computed using the definition of function $f'_k$, and the current input to layer $k$.
        However, layer $k$ requires the factor from layer $k+1$ in order to compute loss gradient according to its own parameters.
        Consequently, the signal (the product of factors accumulated from layer $n$ to current layer $i$) is passed from layer $i+1$ to layer $i$.
        In a more general sense, the gradient signal is passed from the output layer to the input layer, hence the name "backpropagation".

        The move in the gradient direction with step $\alpha$ (the so-called learning rate) is such that:

        \begin{equation}
            \theta_k \leftarrow \theta_k - \alpha \cdot \nabla \ \ell(\theta_k) \  \ \forall k \in \{1, \ldots, n\}  % Formalism: \nabla \ell(\theta_k) should probably be \nabla_{\theta_k}\ell(...)
        \end{equation}

        This step is repeated until one of the stop criteria has been met. For example, the algorithm stops when a maximum number of iterations has been
        reached. However, gradient descent is not the only optimization algorithm that yields satisfying results in practice.
        For example, Limited-memory BFGS~\cite{LBFGS} relies on a second order approximation of the loss function given a limited number of past
        update vectors: this provides a better search direction but in return does not theoretically guarantee that the loss function actually decreases at each
        iteration.

    \subsection{Fully-connected layers}

        A \textit{Multi-layer perceptron} is a neural network composed of multiple layers,
        where each layer's forward pass consists of a linear combination
        followed by an element-wise non-linear activation function.
        Let $X^{(p)} \in \mathbb{R}^{n \times m}$ be the input matrix of layer $p$,  % Watch out: $m$ is already taken for the output dimension.
        $W \in \mathbb{R}^{m \times k}$ the weight matrix,  % Same here
        $b \in \mathbb{R}^{k}$ the bias vector, $n^{(p)}$ the number of inputs to layer $p$  % Also dimensions for layer $p$ should be indexed by parameter $p$.
        and $\sigma$ the non-linear activation function of layer $p$.
        Each layer can than be formalized as follows:

        \begin{equation}
            X_{i, k}^{(p+1)} = \sigma \Big( \sum\limits_{j=1}^{n^{(p)}} X^{(p)}_{i, j} W_{j, k} + b_{k} \Big)  % Matrix notation?
        \end{equation}

        Backpropagation requires to compute the partial derivatives of layer outputs with respect to current layer parameters:

        \begin{align}
            \frac{\partial X_{i, k}^{(p+1)}}{\partial W_{j, k}} & = \sigma' \Big( \sum\limits_{j=1}^{n^{(p)}} X_{i, j} W_{j, k} + b_{k} \Big) \ X_{i, j}^{(p)} \\
            \frac{\partial X_{i, k}^{(p+1)}}{\partial b_{k}} & = \sigma' \Big( \sum\limits_{j=1}^{n^{(p)}} X_{i, j} W_{j, k} + b_{k} \Big)
        \end{align}

        where $Y \in \mathbb{R}^2$ is the output matrix and $\sigma'(x)$ is the derivative of $\sigma(x)$,  % Why mention $Y$ here? Also $Y \in \mathbb R^2$?
        typically $\sigma(x) (1 - \sigma(x))$ for the sigmoid function.

        Multi-layer perceptrons have been proved to be Universal Approximators~\cite{hornik1991approximation},
        meaning that they can approximate feedforward prediction functions that minimize any training loss (loss function computed on the training set).
        However, this fact does not inform about the type of non-linear function to use in order to minimize
        a given loss function. More importantly, this does not guarantee that the model will perform well on unseen examples.
        Indeed, high representational power is required when the classification task is abstract.
        To overcome this problem and lower the validation loss as much as possible, data scientists usually stack more layers on top of each other,
        but this may imply high computational requirements. Convolutional layers are used instead of dense weight matrices.

    \subsection{Convolutional layers} \label{convlayers}

        One of the major advances in semantic segmentation
        is due to Convolutional Neural Networks (CNNs)~\cite{DBLP:journals/corr/Garcia-GarciaOO17}.
        A CNN is an artificial neural network made of a stack of neural layers~\cite{lecun1998gradient}. One characteristic of CNNs is the
        presence of convolutional filters that map raw data to more abstract features. Each filter (or kernel) is locally connected to its output unit, which
        allows the convolutional layer to capture some local information about the inputs, as opposed to fully-connected layers that don't take any spatial
        information into account when passing data forward. This procedure is inspired by the notion of receptive field introduced
        by Hubel and Wiesel~\cite{Hubel1962}.

        Weights are no longer stored in a bidimensional matrix since all inputs are no longer connected to each neuron of the current layer.
        Instead, each neuron is connected to a certain neighborhood of inputs. In this way, the network drastically reduces its number of parameters
        but still takes the spatial dependence of the data into account. If the convolutional layer is designed for processing multi-channel images for example,
        the parameters will be stored in a 4-dimensional tensor. Let $W \in \mathbb{R}^{b \times h \times w \times n_c}$ be the weights of the convolutional filters,
        $X^{(p)} \in \mathbb{R}^{b \times h_b \times w_b \times n_c}$ the input images of layer $p$, $b \in \mathbb{R}$ the bias vector
        and $X^{(p+1)} \in \mathbb{R}^{b \times (\floor{(h_b - h) / \beta_1} + 1) \times (\floor{(w_b - w) / \beta_2} + 1) \times n_f}$  % \beta_1 and \beta_2 should defined here, not after the formula.
        the output feature maps. Let's consider the relation between the input images and the output feature maps:

        \begin{equation} \label{eq:conv2D}
            X_{i, j, k, l}^{(p+1)} = \sigma \Big( \sum\limits_{\alpha=1}^h \sum\limits_{\delta=1}^w
                \sum\limits_{c=1}^{n_c} W_{j, \alpha, \delta, c} X_{i, k+\beta_1 \alpha, l+\beta_2 \delta, c}^{(p)} + b_{j} \Big)
        \end{equation}

            where $i$ is the image identifier, $j$ is the filter index, $n_c$ is the number of channels, $h$ is the filter height, $w$ is the filter width
            and $(\beta_1, \beta_2)$ are the strides (vertical and horizontal distances between neighboring pixels in the neighborhood connected to a same neuron).
            Partial derivatives are simply given by:  % simply? Not sure it's necessary.

        \begin{align}  % /!\ \alpha is an index of the parameter for the partial derivative while also being an index of summation in the RHS.
            \frac{\partial X_{i, j, k, l}^{(p+1)}}{\partial W_{j, \alpha, \delta, c}} & =  % Maybe use commands \pd or \dpd from package commath for partial derivatives
                \sigma' \Big( \sum\limits_{\alpha=1}^h \sum\limits_{\delta=1}^w \sum\limits_{c=1}^{n_c} W_{j, \alpha, \delta, c}
                X_{i, k+\beta_1 \alpha, l+\beta_2 \delta, c}^{(p)} + b_{j} \Big) X_{i, k+\beta_1 \alpha, l+\beta_2 \delta, c} \\
            \frac{\partial Y_{i, j, k, l}}{\partial b_{j}} & =  % Why partial derivative of Y and not X?
                \sigma' \Big( \sum\limits_{\alpha=1}^h \sum\limits_{\delta=1}^w \sum\limits_{c=1}^{n_c} W_{j, \alpha, \delta, c}
                X_{i, k+\beta_1 \alpha, l+\beta_2 \delta, c}^{(p)} + b_{j} \Big)
        \end{align}

        Just as in the case of fully-connected layers, the computations for the signal propagation are not shown because this report is intended to remain brief.
        Contrary to neural networks, it must be noted that random forest implementations are rarely equipped with convolutional filters or even multivariate
        splits. Even in computer vision applications, univariate decision trees are the most frequently used trees in ensemble learners.

        \todo{Double check indices and whether symbols have been correctly defined}

    \subsection{Activation functions}

        An activation function describes the output value of a neuron and is biologically inspired. It is a mathematical representation
        of the level of action potential sent along its axon. More formally, it is a non-linear scalar function that takes a scalar as input.
        The presence of activation functions in neural networks along with fully-connected layers allows them to increase their representational
        power. Indeed, a stack of fully-connected layers without activation functions would have the same representational power as a single
        fully-connected layers, since a linear combination of linear combinations is itself a linear combination. Thus, activation functions
        help to actually build a hierarchical representational of the data by folding the hyperplane containing the data points multiple times  % folding the hyperplane? In order to talk about hyperplanes, you need to be in an affine space and especially you require the projection to lie on a $n-1$ dimensional space which is highly likely not the case. Maybe just say "non-linearly modify the geometry of the projected space". But using "folding" formally would require to talk about manifolds.
        and at each layer.

        However, not every activation function is suitable for backpropagation and one of the reasons for the success of deep learning is the low
        computation requirements for the gradients. Most of the activation functions are non-prametric and element-wise, which makes it easy
        to compute the signal during backward pass.

        The best known activation function is the sigmoid function $\sigma(x)$.
        It has the property to have a derivative $\sigma'(x)$ expressed as a function of $\sigma(x)$,
        which speeds up computation times, assuming that the neural outputs are cached.

        \begin{equation}
            \begin{split}
                \sigma(x) & = \frac{1}{1 + \smallexp{-x}} = \frac{\smallexp{x}}{1 + \smallexp{x}} \\
                \sigma'(x) & = \sigma(x) (1 - \sigma(x))
            \end{split}
        \end{equation}

        However, LeCun~\cite{efficientBackprop} does not recommend standard sigmoid functions because normalizing
        activation functions generally ensure better performance.
        For this reason, the hyperbolic tangent is suitable because its outputs are centered around zero.
        Also, its derivative $\tanh'(x)$ is expressed as a function of $\tanh(x)$ which is computationally convenient.
        Finally, an additional linear term can be added in order to
        avoid flat areas, leading to an activation function of the following form: $f(x) = \tanh(x) + ax$.

        \begin{equation}
            \begin{split}
                \tanh(x) & = \frac{\smallexp{x} - \smallexp{-x}}{\smallexp{x} + \smallexp{-x}} = \frac{\smallexp{2x} - 1}{\smallexp{2x} + 1} \\
                \tanh'(x) & = 1 - \tanh^2(x)
            \end{split}
        \end{equation}

        Assuming that target values are in the set $\{-1, 1\}$ in the framework of binary classficaition,
        the hyperbolic tangent can be linearly modified to obtain a new function of the form $f(x) = 1.7159 \tanh(\frac{2}{3} x)$.
        Such an activation function is profitable because, has its second derivative maximized at $x = -1$ and $x = 1$, avoiding
        saturation effects.

        The chain rule informs us that the gradient of a given layer is factorized as a product of vectors/matrices computed by previous layers.
        Because the norm of gradients w.r.t. activation functions is always less than one for both tanh and standard sigmoid,  % norm w.r.t. activation function? norm w.r.t. something? Especially w.r.t. a function?
        deep architectures are often subject to vanishing gradients. Linear rectifier units (ReLU) are piecewise linear functions designed to solve
        these issues by keeping positive inputs unchanged. Let's note that ReLU is not differentiable at $x = 0$ but inputs are rarely zero in practice.

        \begin{equation}
            \begin{split}
                \text{ReLU}(x) & = \max{(x, 0)} \\
                \text{ReLU}'(x) & =
                \begin{cases}
                    1 & \text{if } x > 0 \\
                    0 & \text{if } x < 0
                \end{cases}
            \end{split}
        \end{equation}

        The outputs of a neural network are often desired to sum to one, especially when the classification task is to assign each class to a probability
        conditionally to the network's input. In the case where there are $m$ classes, the output layer is composed of $m$ neurons where the activation
        function associated to neuron $i$ is given by:

        \begin{equation}
            \begin{split}
                \sigma(x^{(i)}) & = \frac{\smallexp{x^{(i)}}}{\sum\limits_{k=1}^m \smallexp{x^{(k)}}} \\
                \sigma'(x^{(i)}) & = \sigma(x^{(i)}) (1 - \sigma(x^{(i)}))
            \end{split}
        \end{equation}

        where $x^{(i)}$ is the component $i$ of the output vector.  % Why write components as supscript and not subscript here?
        This function is identical to the Boltzmann distribution introduced in section \ref{potts}.


    \subsection{Batch normalization} \label{batchnorm}

        According to Ioffe and Szegedy~\cite{DBLP:journals/corr/IoffeS15}, deep neural networks are subject to a phenomenon
        called \textbf{internal covariate shift}. When the learning rate is too large, the distribution of a layer's output
        is drastically altered, making it difficult to train the next layer since the latter is constantly adapting to the new
        distribution. Batch normalization helps dealing with this issue and allows us to run the training algorithm with less
        careful parameter initialization and larger learning rate.

        When the network is training with batch learning, its parameters are updated at every batch. Therefore, the distribution
        of each layer's output is changed at each batch. This is the reason for using the statistics of each batch individually
        to normalize the data between layers.

        \begin{table}[H]
            \centering
            \begin{tabular}{|l|c|c|}
                \hline
                Name & Number of samples involved & Formula \\
                \hline
                \hline
                Average (true) gradient & $N$ & $\frac{1}{N} \sum\limits_{i=1}^N \nabla L(f_{\theta}(X_i))$ \\
                \hline
                Batch gradient & $\vert B \vert$ & $\frac{1}{\vert B \vert} \sum\limits_{i \in B} \nabla L(f_{\theta}(X_i))$ \\
                \hline
                Sample gradient & $1$ & $\nabla L(f_{\theta}(x))$ \\
                \hline
            \end{tabular}
            \captionof{table}{Types of gradients and gradient approximations used in common optimization methods.}
            \label{gradients}
        \end{table}

        \todo{Backpropagating sample gradients in fully-convolution networks}

        \begin{equation}
            \begin{split}
                \mu_{\mathcal{B}} & = \frac{1}{\vert\mathcal{B}\vert} \sum\limits_{i=1}^{\vert\mathcal{B}\vert} x_i \\
                \sigma_{\mathcal{B}}^2 & = \frac{1}{\vert\mathcal{B}\vert} \sum\limits_{i=1}^{\vert\mathcal{B}\vert} (x_i - \mu_{\mathcal{B}}^2) \\
                \hat{x}_i & \leftarrow \gamma \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}} + \beta
            \end{split}
        \end{equation}

        Optimize $\beta, \gamma$ with backpropagation. \todo{}

    \subsection{Regularization}

	From an optimization perspective, regularization is a penalty used to prevent
	parameters from growing arbitrarily big during training.
	According to Occam's law of parsimony, simpler hypotheses should be privileged over more complex ones.
	Therefore, when the neural architecture involves a large number of free parameters
	in the presence of relatively few data samples,
	regularization helps reducing parameters importance and converging to less arbitrary parameter values.
	From a Bayesian perspective, regularization provides a prior distribution over the model parameters.
	In Bayes formula, the posterior $P(\theta \vert X, \alpha)$ is a function of both the prior
	$P(\theta \vert \alpha)$ and the likelihood of the data $P(X \vert \theta, \alpha)$ under model $\theta$.

	\begin{equation}
	    P(\theta \vert X, \alpha) = \frac{P(X \vert \theta, \alpha)\,P(\theta \vert \alpha)}{P(X \vert \alpha)}
	\end{equation}

	The relation between the loss function of a neural network and Bayes formula can be established
	by proving the two following points:

	\begin{itemize}
	    \item The log-likelihood of the data is equal to the negative cross-entropy.
	    \item The regularization term is proportional to the prior distribution of the parameters.
	\end{itemize}

	The first part is easy to show since negative log-likelihood can be obtained from binary cross-entropy:
	\begin{align}
		CE(\hat{y}, y) & = - \log{\prod\limits_{i=0}^n P(\hat{y_i})^{y_i}}  \\
		& = -\sum\limits_{i=0}^n y_i \log{\hat{y}} + (1 - y_i) \log{1 - \hat{y}}
	\end{align}
	This allows us to provide a statistical interpretation of the loss function.
	Regarding priors, $L_1$ and $L_2$ regularizations are going to be introduced in the following two sections.

	\subsubsection{$L_1$ regularization}

	   Adding a $L_1$ regularization term to the loss function reduces to providing a Laplacian
	   prior on model parameters.

	   \begin{align}
	       \max_{\theta}\, \log{P(\theta \vert \eta, b)}
		   & = \max_{\theta}\, \log{\prod\limits_{i=1}^m \, \frac{1}{2b_i}\,
		   \exp{\frac{- \abs{\theta_i - \eta_i}}{b_i}}} \\
		   & = \max_{\theta}\, \sum\limits_{i=1}^m \frac{- \abs{\theta_i - \eta_i}}{b_i} - \log{2b_i} \\
		   & = \min_{\theta}\, \sum\limits_{i=1}^m \abs{\theta_i - \eta_i}
	   \end{align}

	   By setting vector $\eta \in \mathbb{R}^m$ to $0$, the resulting regularization term
	   takes its final well-known form $\sum\limits_{i=1}^m \abs{\theta_i}$.

	\subsubsection{$L_2$ regularization}

	   $L_2$ regularization acts as a Gaussian prior on model parameters.
	   This can be highlighted by setting the probability density function of the Gaussian
	   distribution as the prior and show that the regularization term is proportional
	   to the logarithm of the product of priors.

	   \begin{align}
               \max_{\theta}\, \log{P(\theta \vert \eta, \sigma)}
		   & = \max_{\theta}\, \log{\prod\limits_{i=1}^m \, \frac{1}{\sqrt{2\pi\sigma^2}}
		   \exp{-\frac{(\theta - \eta)^2}{2\sigma^2}}} \\
		   & = \max_{\theta}\, \sum\limits_{i=1}^m \, -\frac{(\theta_i - \eta_i)^2}{2\sigma^2}
		   - \log{\sqrt{2\pi\sigma^2}} \\
		   & = \min_{\theta}\, \sum\limits_{i=1}^m \, (\theta_i - \eta_i)^2
           \end{align}

	   Again, by setting vector $\eta \in \mathbb{R}^m$ to $0$, the regularization term takes its
	   final form $\sum\limits_{i=1}^m \theta_i^2$.

    \subsection{Optimization algorithms}

        The gradient vector is obtained by concatenating the gradients w.r.t.
	each layer's parameter vector. This final gradient vector
        gives an improvement direction, but a decrease of the loss function
	is guaranteed by moving by a arbitrary small step in the parameter space.

        \todo{Overview: \cite{DBLP:journals/corr/Ruder16}}

        \todo{Adam: \cite{DBLP:journals/corr/KingmaB14}}

        \todo{Adadelta: \cite{DBLP:journals/corr/abs-1212-5701}}

        \todo{L-BFGS: \cite{LBFGS}}

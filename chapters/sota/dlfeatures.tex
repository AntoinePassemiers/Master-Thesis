\subsection{Input features} \label{inputfeatures}

    \subsubsection{Global features}

        \paragraph{Protein length}

      	    Protein length is computed as the number of residues in the protein sequence.
    	    It provides complementary information that convolutional layers cannot capture.
     	    Indeed, fully-connected neural networks have the ability to handle arbitrary-sized
    	    features maps at the cost of not knowing the dimensionality of their inputs.
    	    Injecting protein length as a supplementary global feature may help the model
    	    to infer the maximum distance in long-range contacts.

	   \paragraph{Effective number of sequences} \label{meff}

    	    The number of effective sequences is equal, w.r.t. to a given threshold,
    	    to the number of non-redundant sequences in the set of homologous sequences.
    	    It provides a bound on the potential performance of the DCA methods involved
    	    in the pipeline.

            \begin{equation}  % Formalism: unclear. Is this \#\{j \in \{1, \ldots, m\} : r(s_i, s_j) \geq \tau\} or is this 1 if \forall j \in \{1, \ldots, m\} : r(s_i, s_j) \geq \tau and 0 if \exists j \in \{1, \ldots, m\} such that r(s_i, s_j) < \tau?
                M_{eff} = \sum_{i=1}^m w_i = \sum_{i=1}^m \frac{1}{\#\{r(s_i, s_j) \ge \tau \, \, \, \forall j \in \{1, \dotsc, m\}\}}
            \end{equation}

            $w_i$ is the weight associated to homologous sequence $i$.
            $r(s_i, s_j)$ is the identity rate between sequences $s_i$ and $s_j$, in other words
            the number of matching residues (including gaps) divided by the total number of positions,
            which is assumed to be equal in both sequences.

    \subsubsection{1-dimensional features}

        \paragraph{One-hot encoded sequence}

            Given an amino acid sequence $\{s_1, s_2, \ldots,
            s_L\}$ of size L, its one-hot encoded version
            is a matrix $X \in \{0, 1\}^{L \times 21}$
            where $x_{ia}$ is one if $s_i = a$.

        \paragraph{Solvent accessibility prediction}

            \todo{}

            \begin{figure}[H]
                \begin{center}
                    \includegraphics[height=5cm, keepaspectratio]{imgs/accessibility.png}
                    \caption{Accessible surface, obtained by "rolling" a probe sphere (a molecule
                        of solvent, colored in orange) on the Van der Waals surface of a biomolecule
                        (colored in blue).}
                    \label{architecture}
                \end{center}
            \end{figure}

            \todo{define Relative Solvent Accessibility}
            \todo{cite RaptorX-property}
            RSA prediction is 3-state. A residue is either:
            \begin{itemize}
                \item Buried (B), when RSA is below 10\%.
                \item Intermediate (I), when RSA is between 10\% and 40\%.
                \item Exposed (E), when RSA is above 40\%.
            \end{itemize}


        \paragraph{Predicted secondary structure prediction}

            As defined by DSSP, there are 8 different states for encoding
            secondary structure: H (alpha-helix),
            G (310 helix), I (pi-helix), E (beta-strand), B (beta-bridge),
            T (beta-turn), S (high curvature loop) and L (irregular loop).
            In practice, secondary structure prediction is either 3-state
            or 8-state. \todo{cite RaptorX-property}
            When the number of labels is limited to three,
            the predictor only focuses on beta-strands (E), alpha-helices (H)
            and a third state which is the union of the six remaining states (C).

            Formally, a $m$-state secondary structure prediction is a matrix
            $S \in [0, 1]^{L \times m}$ where element $S_{i,j}$ is the probability
            of residue $i$ being in conformation $j$,
            with each row summing to one.

        \paragraph{Region disorder prediction}

            Region disorder prediction is a vector $D \in [0, 1]^L$
            where $D_i$ is the probability of residue $i$ being in a region
            of missing residues in the X-ray 3D structure. Residues with high
            probability are said to be disordered.

        \paragraph{Amino acid frequencies}

            Amino acid frequencies are position-specific features that can be efficiently computed.
            Let $S \in \{0, \ldots, \naatypes\}^{M \times L}$ be a
            MSA matrix containing $M$ sequences aligned to a target
            sequence of length $L$. Then amino acid frequencies can be arranged
            in a matrix $F \in \mathbb{R}^{L \times \naatypes}$
            where element $F_{ia}$ is computed as follows:

            \begin{equation}
                F_{ia} = \frac{1}{M} \sum\limits_{k=1}^M \delta(S_{ki}, a)
            \end{equation}

        \paragraph{Position-Specific Scoring Matrix}

            \todo{PSI-PRED: \cite{jones1999protein}}

        \paragraph{Atchley factors}

            \todo{Atchley: \cite{Atchley2005}}

        \paragraph{Self-information}

            In information theory, self-information is the amount of information, in bits,
            obtained by observing a random variable. In particular, let $x_{ij} \in \{0, 1\}$ be
            a binary variable indicating the presence of an amino acid of type $j$ at site $i$.
            The self-information suggested by Michel et al~\cite{Michel383133} can be formalized
            with the following equation:

            \begin{equation}
                I_{ij} = \log_2 (p_{ij} / \langle p_i \rangle)
            \end{equation}

            where $p_{ij}$ is the probability of observing amino acid $j$ at site $i$ among all residues
            of given MSA, and $\langle p_j \rangle$ is the frequency of amino acid $j$
            in the Uniref50 dataset.

        \paragraph{Partial entropies}

            \todo{ \cite{Michel383133}}

            \begin{equation}
                S_i = p_i \log_2 (p_i / \langle p_i \rangle)
            \end{equation}

    \subsubsection{2-dimensional features}

        \paragraph{Mutual Information and Normalized Mutual Information}

            Following the formalism described in \cite{Michel383133}, MI is described as:

            \begin{equation}
                MI(x, y) = \sum\limits_{x, y} P(x, y) \log \Big( \frac{P(x, y)}{P(x) P(y)} \Big)
            \end{equation}

            \begin{equation}
                NMI(x, y) = \frac{MI(x, y)}{\sqrt{S(x) S(y)}}  % S(x) and S(y)?
            \end{equation}

            APC is applied to both MI and NMI.  % APC?

        \paragraph{Cross-entropy}

            Cross-entropy is computed in \cite{Michel383133} using the following formula:

            \begin{equation}
                H(x, y) = S(x) + S(y) - MI(x, y)
            \end{equation}

        \paragraph{Contact potential}

            \todo{}

        \paragraph{Evolutionary couplings}

            Predictions from GaussDCA, plmDCA or PSICOV can be used as input to the deep
            learning approach. Only top predicted contacts are informative,

        \paragraph{Covariance}

            Covariance matrices are computed as in equation \ref{covariance} of the section
            about Gaussian graphical models (see section \ref{graphicalmodels}).
            In PSICOV~\cite{doi:10.1093/bioinformatics/btr638}, the inferred covariance
            matrix is averaged across the dimension of amino acid types.
            In DeepCov~\cite{doi:10.1093/bioinformatics/bty341}, the full matrix is used
            as input to the supervised model.

    \subsection{Features for the proposed approach}

        The lines which will follow are a discussion about the features to use as inputs
        to the model. At the outset, it should be noted that all models rely on
        two-dimensional features. Those features are either predictions from another
        predictor, covariance matrices or correlated mutations.
        However, using intermediary predictions requires installing additional software.
        PSICOV is written in C and can be recompiled. The official implementation of
        plmDCA was originally made in Matlab, forcing researchers to add a whole raft
        of external tools. plmDCA, as well as GaussDCA now have a Julia implementation,
        making them more accessible. The proposed approach incorporates all three
        predictors in its pipeline.

        \todo{}

        \begin{figure}[H]
            \begin{center}
                \includegraphics[width=\textwidth, keepaspectratio]{imgs/features.png}
                \caption{Features used in state-of-the-art deep learning approaches.
                Feature extraction methods that rely on external tools (excluding
                MSA tools) are highlighted in red.}
                \label{features}
            \end{center}
        \end{figure}


\chapter{State-of-the-art}

\section{Direct Coupling Analysis}

    \subsection{Potts model} \label{potts}

        Experts have long thought that the three-dimensional structure of proteins
        is related to their amino acid composition. However, homologous proteins
        are subject to a high variability with regards their amino acid composition.
        Structural conservation across evolution induces stresses on these variability
        patterns in such a manner that spatially close residues have a restricted set
        of acceptable amino acid substitutions. Therefore, neighbouring residues
        are forced to coevolve, which results in correlated mutations~\cite{Morcos2014}.

        Which makes structural prediction complex is that correlated mutations can
        be caused either by low data quality or availability manifested by the absence
        of a large number of homologous proteins, a high redundancy among homologous
        proteins or various phylogenetic effects. In addition to all these sources
        of error, a correlation between two residue sites may also be mediated by a third
        residue located at a different site and having direct correlations with the two
        other ones.

        The core idea of Direct Coupling Analysis (DCA\index{DCA}) is to disentangle direct
        correlations and indirect correlations related to residues at intermediary positions.
        Potts model allows to perform this disentanglement through inverse
        statistical mechanics~\cite{PhysRevE.87.012707}.

        DCA takes ideas from the Potts model~\cite{wu1982potts}, a generalization of the
        problem originally stated by Cyril Domb, called the Ising problem.
        The Potts model used in DCA involves two types of parameters:
        \begin{itemize}
            \item The pairwise \textbf{couplings} $J$ are the values of interest
                for protein contact prediction since $J_{ij}(s_i, s_j)$ is defined
                as the predicted distance between residues $i$ and $j$ in the protein structure,
                knowing that they are in states $s_i$ and $s_j$ respectively~\cite{morcos2011direct}.
            \item The \textbf{fields} $h$ are local biases of the Boltzmann distribution.
        \end{itemize}

        Evolutionary-related sequences are modelled by the distribution
        given by the Maximum-Entropy Principle. Among the family of distributions that are suited
        for proteins (which are discrete sequences), the one that maximizes entropy is the
        Boltzmann distribution, with the following probability mass functiony~\cite{morcos2011direct}:

        \begin{equation}
            P(s \vert J, h) = \frac{1}{Z} \exp{\sum\limits_{i=1}^L \sum\limits_{j=i+1}^L J_{ij}(s_i, s_j) + \sum\limits_{i=1}^L h_i(s_i)}
        \end{equation}

        where couplings $J$ and fields $h$ are the model parameters, $s$ is an amino acid sequence and
        $Z$ is a normalization factor called partition function ensuring
        that the sum $\sum_s P(s \vert J, h)$ over all lexicographically
        possible sequences is equal to one.
        Let's note that residue $s_i$ at site $i$ is defined over the alphabet $\{ 1, \dotsc, q \}$, where $q$ is the number of possible
        residue states (namely its amino acid type).

    \subsection{Exact inference is hard}

        Given a multiple sequence alignment containing $M$ sequences, a na\"\i
        ve approach would be to maximize its log-likelihood:

        \begin{equation}
            \begin{split}
                \log{L}(J, h \vert s^{(1)}, \dotsc, s^{(M)}) & = \sum\limits_{k=1}^M \log P(s^{(k)} \vert J, h) \\
                & = \sum\limits_{k=1}^M \log \Bigg( \frac{1}{Z} \exp{\sum\limits_{i=1}^L
                    \sum\limits_{j=i+1}^L J_{ij}(s_i^{(k)}, s_j^{(k)}) + \sum\limits_{i=1}^L h_i(s_i^{(k)})} \Bigg) \\
                & = -M \log(Z) + \sum\limits_{k=1}^M \Big( \sum\limits_{i=1}^L \sum\limits_{j=i+1}^L J_{ij}(s_i^{(k)}, s_j^{(k)})
                    + \sum\limits_{i=1}^L h_i(s_i^{(k)}) \Big)
            \end{split}
        \end{equation}

        with the following partial derivatives:

        \begin{equation}
            \begin{split}
                \frac{\partial}{\partial J_{ij}(a, b)} \log{L}(J, h \vert s^{(1)}, \dotsc, s^{(M)}) & =
                    -M \frac{\partial \log(Z)}{\partial J_{ij}(a, b)} + \sum\limits_{k=1}^M \Big[s_i^{(k)} = a\Big] \Big[s_j^{(k)} = b\Big] \\
                 \frac{\partial}{\partial h_{i}(a)} \log{L}(J, h \vert s^{(1)}, \dotsc, s^{(M)}) & =
                    -M \frac{\partial \log(Z)}{\partial h_{i}(a)} + \sum\limits_{k=1}^M \Big[s_i^{(k)} = a\Big]
            \end{split}
        \end{equation}

        where $[ \cdot ]$ are Iverson brackets.

        However, there is no straightforward method to compute the partition function $Z$ or $L$'s gradient for large
        systems due to the discrete nature of amino acid sequences.
        Indeed, $Z$ contains $21^L$ terms for systems with 21 possible symbols (amino acid types + gap)
        and sequences of length $L$.
        For this aim, several methods like Mean-Field (mfDCA)~\cite{MorcosE1293}, Message Passing (mpDCA)~\cite{Weigt2009},
        Pseudo-Likelihood Maximization (plmDCA)~\cite{EKEBERG2014341}
        or Multivariate Gaussian Modeling (GaussDCA)~\cite{10.1371/journal.pone.0092721}
        have been developed.

    \subsection{Pseudo-Likelihood Maximization}

        \index{plmDCA}plmDCA~\cite{EKEBERG2014341} addresses the problem of estimating
        the partition function by maximizing the pseudo-loglikelihood instead of the loglikelihood.
        The pseudo-loglikelihood can be expressed as the sum of loglikelihoods $\log{L}(J_r, h_r)$,
        where each $\log{L}(J_r, h_r)$ is computed
        at a single site $r$. The method thus assumes the conditional
        independence between variables belonging to different sites.
        However, the partition function at a given site can be easily computed as a sum of
        21 terms since a state can take 21 possible values at a given position.
        More formally, the penalized loglikelihood at site $r$ is given by:

        \begin{equation}
            \begin{split}
                \log{L^{(reg)}}(J_r, h_r) = & -\frac{1}{M_{eff}} \sum\limits_{k=1}^M w_k \Bigg( h_r(s_r^{(k)})
                    + \sum\limits_{i \neq k}^L J_{ri}(s_r^{(k)}, s_i^{(k)}) - \log(Z_r) \Bigg) \\
                & + \lambda_h \norm{h_r}_2^2 + \lambda_J \norm{J_r}_2^2 \\
                \text{where} \ \ \ \ \ \ \ \ Z_r = & \sum\limits_{a=1}^q \exp{h_r(a) + \sum\limits_{i \neq r} J_{ri}(a, s_i^{(k)})}
            \end{split}
        \end{equation}

        It can be observed that the formula contains a $L_2$ penalty term for both fields and couplings,
        and that each log-probability is weighted by a protein weight $w_k$.
        The latter is computed as the protein contribution to set the set of effective sequences,
        as described in the section of $M_{eff}$~\ref{meff}.
        It must be observed that the optimization procedure is called asymmetric pseudolikelihood maximization because
        each matrix $J(i, j)$ is supposed to be symmetric and in practice estimated independently.
        This allows plmDCA to run in parallel by optimizing $\log{L^{(reg)}}(J_r, h_r)$ each on a different core.

        After maximizing the pseudo-loglikehood in parallel, all remaining information
        that can be explained by the fields are removed from the couplings by applying
        an average sum correction w.r.t. the states:

        \begin{equation}
            \hat{J}_{ij}(a, b) = J_{ij}(a, b) - \frac{1}{q} \sum\limits_{a=1}^q J_{ij}(a, b) - \frac{1}{q} \sum\limits_{b=1}^q J_{ij}(a, b)
            + \frac{1}{q^2} \sum\limits_{a=1}^q \sum\limits_{b=1}^q J_{ij}(a, b)
        \end{equation}

        Then each matrix $J(a, b)$ is symmetrized by simply averaging it with its transpose:

        \begin{equation}
            \hat{J}(a, b) \leftarrow \frac{1}{2} \big( \hat{J}(a, b) + \hat{J}(a, b)^T \big) \ \ \ \forall a, b
        \end{equation}

        Finally, a contact map is obtained by normalizing $\hat{J}$ over the states and applying an average product correction w.r.t. the sites.
        plmDCA shows remarkable performance on diverse sets of proteins with running times competitive with mean-field DCA.


    \subsection{Gaussian Direct Coupling Analysis}

        plmDCA is of state-of-the-art performance, but still requires high computational resources.
        An alternative method is to use GaussDCA\index{GaussDCA}~\cite{10.1371/journal.pone.0092721}, which does
        exact inference wihout having recourse to iterative algorithms.

        In GaussDCA, each variable $x_i \in \{0, 1\}$ indicates whether residue located at locus $i \% L \in \{ 1, \dotsc, L \}$
        is of amino acid type $i / L \in \{1, \dotsc, \naatypes\}$. With such a formalism, each protein is described as a vector
        $x \in \{0, 1\}^{\naatypes\,L}$. The key assumption at the core of the method is to approximate each binary variable $x_i$
        by a continuous Gaussian variable. Let $m$ be the number of sequences in a MSA, $X \in \{0, 1\}^{m \times \naatypes\,L}$
        be the matrix representation of the MSA, and $\mu, \bar{x}$ be respectively, the theoretical and empirical mean vectors associated to $X$.
        The empirical covariance matrix of $X$ is given by:
        \begin{equation}
            \bar{C}_{ij}(X, \mu) = C_{ij}(X, \mu) = \frac{1}{m} \sum\limits_{k=1}^m (x_i^k - \mu_i) (x_j^k - \mu_j)
        \end{equation}

        Similarly to PSICOV\index{PSICOV}~\cite{doi:10.1093/bioinformatics/btr638}, evolutionary couplings are detected by keeping track of direct interactions
        between variables of the system, which can be realized by computing the precision matrix, also known as the inverse of the covariance matrix.
        When matrix $\bar{C}$ is not rank deficient, maximum log-likelihood is attained by setting the theoretical covariance matrix
        $\Sigma$ to $\bar{C}$. However, the empirical covariance matrix rarely has full rank due to the limited number of effective sequences
        in MSAs. The suggested solution is to provide a prior distribution on positive-definite matrices and perform exact Bayesian inference
        to find an invertible covariance matrix.

        \subsubsection{Bayesian inference}

            In Bayesian inference, a hypothesis is favoured over others based on its posterior probability,
            which is proportional to the product of the data log-likelihood under this hypothesis
            and its prior probability. This relation holds in the parametric formulation of Bayes' rule:

            \begin{equation}
                P(\theta \vert X, \alpha) = \frac{P(X, \theta \vert \alpha) P(\theta \vert \alpha)}{P(X \vert \alpha)} \propto
                P(X, \theta \vert \alpha) P(\theta \vert \alpha)
            \end{equation}

            $P(\theta \vert \alpha)$ is a prior distribution, hence a distribution over the parameter space without
            any knowledge about the data $X$. As more and more data becomes available, the newly observed samples can be
            incorporated to the model through the likelihood $P(X, \theta \vert \alpha)$. The likelihood measures how strongly
            the data is explained by the model, given a set of parameter values. $P(X \vert \alpha)$ is called evidence or
            marginal likelihood because it is equal to the data likelihood marginalized over the parameters $\theta$.
            $P(\theta \vert X, \alpha)$ is the posterior distribution, a probability of some parameters given
            the observed data.

            The marginal likelihood is a constant term that varies only with the hyper-parameters $\alpha$. Therefore, the maximum
            a posteriori estimation (MAP) is simply the maximum product between the likelihood and the prior.

        \subsubsection{Prior distribution}

            A reasonable choice for the prior distribution over $\mu$ and $\Sigma$ is the normal-inverse-Wishart (NIW)
            distribution, which is known to be conjugate prior for the Gaussian log-likelihood.
            The prior and the posterior are said to be conjugate if they are of the same form. In the case of NIW prior,
            the posterior is also a NIW distribution. The prior is defined as the product
            $P(\mu, \Sigma) = P(\mu \vert \Sigma)\,P(\Sigma)$, where the prior of the mean vector is defined as a multivariate
            Gaussian distribution:

            \begin{equation}
                P(\mu \vert \Sigma) = \Big(\frac{2 \pi}{\kappa}\Big)^{-\frac{m}{2}} \vert\Sigma\vert^{-\frac{1}{2}}
                \exp{\frac{\kappa}{2} (\mu - \eta)^T \Sigma^{-1} (\mu - \eta)}
            \end{equation}
            where $\kappa$ is the number of prior measurements and $\eta$ the prior mean.

            Let's note that the Gaussian distribution is conjugate to itself. The prior over positive-definite matrices
            (we are interested in invertible covariance matrices exclusively) is defined by the NIW distribution:

            \begin{equation}
                \begin{split}
                    P(\Sigma) & = \frac{1}{Z} \vert\Sigma\vert^{-\frac{\nu + m + 1}{2}} \exp{-\frac{1}{2} \trace{\Lambda \Sigma^{-1}}} \\
                    \text{where } & \quad Z = 2^{\frac {\nu m}{2}} \pi^{\frac{m (m - 1)}{4}} \vert\Lambda\vert^{-\frac \nu2}
                    \prod\limits_{k=1}^{m} \Gamma\Big(\frac{\nu + 1 - k}{2}\Big)
                \end{split}
            \end{equation}

            where $\Gamma$ is Euler's Gamma function, $\nu$ is the degree of freedom and $\Lambda$ is a parameter matrix called scale matrix.

        \subsubsection{Computing the MAP covariance matrix}

            The mean values for the NIW prior are known and equal to $\eta$ and $\frac{1}{\nu - m - 1} \Lambda$, respectively.
            Assuming $\eta'$, $\nu'$ and $\Lambda'$ are the corresponding parameters in the NIW posterior, the mean values are given by
            $\eta'$ and $(1 / (\nu' - m - 1)) \Lambda$, respectively.
            In particular, the matrix $\Lambda'$ that allows the posterior to be conjugate
            with the prior is given by the following relation:
            \begin{equation}
                \Lambda' = \Lambda + n \bar{C} + \frac{\kappa m}{\kappa + m} (\bar{x} - \eta) (\bar{x} - \eta)^T
            \end{equation}
            Finally, the average covariance matrix is computed as:
            \begin{align*}
                \Sigma & = \frac{\Lambda'}{\nu' - m - 1} \\
                & = \frac{\Lambda + n \bar{C} + \frac{\kappa n}{\kappa + n} (\hat{x} - \eta)^T (\bar{x} - \eta)}{\nu + n - m -1} \\
                & = \lambda \Lambda + (1 - \lambda) \bar{C} + \lambda (1 - \lambda) (\bar{x} - \eta)^T (\bar{x} - \eta)
            \end{align*}
            As a viable choice for the hyper-parameter matrix $\Lambda$, one could choose the most trivial one to avoid
            arbitrary choices. To this aim, the covariance
            matrix corresponding to a uniform multivariate distribution is used.

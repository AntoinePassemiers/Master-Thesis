\section{Deep learning}

    \subsection{A definition of deep learning}

        Beyond the trendy words, it is quite difficult to find a consensus
        on the definition of deep learning. The concept is often associated to
        the concept of inferring a high-level representation of the data
        by alternating many times between parameterized functions and non-linearities.
        Deep artificial neural networks serve this purpose well since they are composed of many sets of parameters and a large stack
        (or graph) of mathematical operators linked to an objective function to be optimized.
        Each parameteric operator may rely on a subset of the network parameters.
        
        In most simple cases (e.g. feedforward neural network), the network can be described as a regular stack of operators.
        As a result, the objective function is a composition of all the underlying mathematical operations.
        Such a network is usually trained using the backpropagation algorithm.
        The latter method consists in minimizing the objective function, which usually is a dissimilarity
        measure between what the network
        predicts for a given input and what the human supervisor expects for such input.
        More specifically, backpropagation is an iterative algorithm that evaluates the gradient of the
        objective at each iteration and performs one step in the direction of the steepest descent in the parameter space.
        The algorithm is expected to stop once a global minimum has been reached.
        Formal details about the algorithm are going developped in the next sub-section.

        Deep learning is also often viewed as the ability of a machine to build
        a hierarchical representation of the data by mapping input values to high-level features.
        According to Yoshua Bengio and Yann LeCun, neural networks only exemplify the notion of deep
        architectures. They provided a sufficiently good basis for a definition:

        \begin{quotation}
            Deep architectures are compositions of many layers of adaptive non-linear components,
            in other words, they are cascades of parameterized non-linear modules that contain
            trainable parameters at all levels. Deep architectures allow the representation of wide
            families of functions in a more compact form than shallow architectures, because they
            can trade space for time (or breadth for depth) while making the time-space product
            smaller, as discussed below. The outputs of the intermediate layers are akin to intermediate
            results on the way to computing the final output. Features produced by the lower
            layers represent lower-level abstractions, that are combined to form high-level features
            at the next layer, representing higher-level abstractions~\cite{40d5d7fd62cb44ba934a8a75d4b2b076}.
        \end{quotation}

        This definition seems to be perfectly appropriate for neural networks since they are precisely
        made of linear - and consequently parametric - operations followed by activation functions which
        are non-linear by nature.

    \subsection{The backpropagation algorithm} \label{backpropagation}

        In this sub-section, the backpropagation algorithm is going to be introduced formally in order
        to understand the subtleties of deep learning.
        Let's consider a feedforward neural network containing no cycle.
        Each of its layers can be viewed as a couple $(f_i(\theta_i, X), b_i(\theta_i, S(X)))$,
        where $f_i$ is the forward pass function of layer $i$ used for predicting,
        $b_i$ is the backward pass function, $\theta_i$ is the set of parameters,
        and $X, Y$ are input tensors of shapes compatible with $f_i$ and
        $g_i$, respectively, and $S(X)$ is the signal tensor propagated from next layer back to current layer.
        Let's make the assumption that convolutional layers are two-dimensional and that input instances are image-like data.
        (one-dimensional and three-dimensional convolutions can be described analogously).
        Also, let's consider a particular case of neural network consisting of a stack of neural layers instead of a graph:
        a feedforward neural network.
        Let $b$ be the number of instances in the input tensor (more commonly referred to as the batch size),
        $w$ and $h$ respectively the width and height of the images,
        and $c$ the number of channels. Finally, let $n$ be the number of layers and $m$ be the number of output neurons in the network.
        Knowing this, the output $Y \in \mathbb{R}^{b \times m}$ of the network can now be described as such:

        \begin{equation}
            Y = (\bigcirc_{i=1}^{n} f_{\theta_i})(X)
        \end{equation}

        where $X \in \mathbb{R}^{b \times w \times h \times c}$ and $f_{\theta_i}(X)$
        is syntactic sugar for denoting $f_i(\theta_i, X)$ in a more convenient way. It can be observed that the prediction
        function of the network is basically a large composition of functions.

        Such model is designed to optimize a function reflecting its ability to accurately predict a target value or to abstractly represent the input
        data in a more general sense. Accordingly, let's introduce a generic loss function
        $L(Y): \mathbb{R}^{b \times m} \rightarrow \mathbb{R}$ that measures
        the model's inability to fulfill the given task.
        The loss takes the output $Y$ of the network as input, and represents the objective function to be minimized.
        Using the composition rule and by replacing $Y$ in $L(Y)$, we obtain the following expression:

        \begin{equation}
            \hat{\theta} = \argmin_{\theta \in \Theta} L((\bigcirc_{i=1}^{n} f_{i, \theta_i})(X))
        \end{equation}

        where $\Theta$ is the set of all possible values for the parameter set $\theta = (\theta_1, \ldots, \theta_n)$.
        The generic task of minimizing a scalar continuous function can be achieved
        using numerous continuous optimization techniques among gradient descent algorithms~\cite{DBLP:journals/corr/Ruder16}
        or quasi-Newton methods~\cite{LBFGS}, as will be detailed in section~\ref{opti}.
        In practice, gradient descent approaches require more iterations to converge to a satisfying solution,
        but are easier to implement. Also, contrary to quasi-Newton methods,
        they don't require to implicitly compute the hessian matrix of the loss
        function according to the network parameters, which makes them less computation-intensive.
        
        Let's consider the optimization of the loss function in the gradient descent framework.
        The loss function is minimized by moving in the parameter space in the direction of the loss gradient, with
        a step proportional to the learning rate (a parameter either determined empirically
        or adjusted dynamically during optimization phase).
        Luckily, since we are regarding our neural network as a stack of layers
        (viewed as a composition of functions), the gradient computation can be decomposed using the chain rule:

        \begin{equation} \label{eq:backprop}
            \frac{\partial (f \circ g)}{\partial w}(X) = \nabla f(g(X)) \cdot \frac{\partial g}{\partial w}(X)
        \end{equation}

        where $w$ can be any parameter of the network.
        Knowing this, the gradient of the loss function w.r.t. to the parameter set $\theta_p$
        of layer $j$ (for any layer $j$ with learnable parameters),
        can be decomposed as the following product:

        \begin{equation} \label{eq:loss}
            \prod_{k=1}^p f_{\theta_k}'\left((\bigcirc_{i=1}^{k} f_{\theta_i})(X)\right) \cdot L'\left((\bigcirc_{j=1}^{n} f_{\theta_j})(X)\right)
        \end{equation}  % Using $i$ as upper bound of composition is very weird.

        Each factor $k$ of the product can be computed using the definition of function $f'_k$, and the current input to layer $k$.
        However, layer $k$ requires the factor from layer $k+1$ in order to compute loss gradient according to its own parameters.
        Consequently, the signal (the product of factors accumulated from layer $n$ to current layer $i$) is passed from layer $i+1$ to layer $i$.
        In a more general sense, the gradient signal is passed from the output layer to the input layer, hence the name "backpropagation".

        The move in the gradient direction with step $\alpha$ (the so-called learning rate) is such that:

        \begin{equation}
            \theta_k \leftarrow \theta_k - \alpha \cdot \nabla_{\theta_k}L(X) \quad \forall k \in \{1, \ldots, n\}
        \end{equation}

        This step is repeated until one of the stop criteria has been met. For example, the algorithm stops when a maximum number of iterations has been
        reached. However, gradient descent is not the only optimization algorithm that yields satisfying results in practice.
        For example, Limited-memory BFGS~\cite{LBFGS} relies on a second order approximation of the loss function given a limited number of past
        update vectors: this provides a better search direction but in return does not theoretically guarantee that the loss function actually decreases at each
        iteration.

    \subsection{Fully-connected layers}

        A Multi-layer perceptron is a neural network composed of multiple layers,
        where each layer's forward pass consists of a linear combination of the inputs
        followed by an element-wise non-linear activation function.
        Let $X^{(p)} \in \mathbb{R}^{n \times m}$ be the input matrix of layer $p$,  % Watch out: $m$ is already taken for the output dimension.
        $W \in \mathbb{R}^{m \times k}$ the weight matrix,  % Same here
        $b \in \mathbb{R}^{k}$ the bias vector, $n^{(p)}$ the number of inputs to layer $p$  % Also dimensions for layer $p$ should be indexed by parameter $p$.
        and $\sigma$ the non-linear activation function of layer $p$.
        Each layer can than be formalized as follows:

        \begin{equation}
            X_{i, k}^{(p+1)} = \sigma \Big( \sum\limits_{j=1}^{n^{(p)}} X^{(p)}_{i, j} W_{j, k} + b_{k} \Big)  % Matrix notation?
        \end{equation}

        Backpropagation requires to compute the partial derivatives of layer outputs with respect to current layer parameters:

        \begin{align}
            \frac{\partial X_{i, k}^{(p+1)}}{\partial W_{j, k}} & = \sigma' \Big( \sum\limits_{j=1}^{n^{(p)}} X_{i, j} W_{j, k} + b_{k} \Big) \ X_{i, j}^{(p)} \\
            \frac{\partial X_{i, k}^{(p+1)}}{\partial b_{k}} & = \sigma' \Big( \sum\limits_{j=1}^{n^{(p)}} X_{i, j} W_{j, k} + b_{k} \Big)
        \end{align}

        where $Y \in \mathbb{R}^2$ is the output matrix and $\sigma'(x)$ is the derivative of $\sigma(x)$,  % Why mention $Y$ here? Also $Y \in \mathbb R^2$?
        typically $\sigma(x) (1 - \sigma(x))$ for the sigmoid function.

        Multi-layer perceptrons have been proved to be Universal Approximators~\cite{hornik1991approximation},
        meaning that they can approximate feedforward prediction functions that minimize any training loss (loss function computed on the training set).
        However, this fact does not inform about the type of non-linear function to use in order to minimize
        a given loss function. More importantly, this does not guarantee that the model will perform well on unseen examples.
        Indeed, high representational power is required when the classification task is abstract.
        To overcome this problem and lower the validation loss as much as possible, data scientists usually stack more layers on top of each other,
        but this may imply high computational requirements. Convolutional layers are used instead of dense weight matrices.

    \subsection{Convolutional layers} \label{convlayers}

        One of the major advances in semantic segmentation
        is due to Convolutional Neural Networks (CNNs)~\cite{DBLP:journals/corr/Garcia-GarciaOO17}.
        A CNN is an artificial neural network made of a stack of neural layers~\cite{lecun1998gradient}. One characteristic of CNNs is the
        presence of convolutional filters that map raw data to more abstract features. Each filter (or kernel) is locally connected to its output unit, which
        allows the convolutional layer to capture some local information about the inputs, as opposed to fully-connected layers that don't take any spatial
        information into account when passing data forward. This procedure is inspired by the notion of receptive field introduced
        by Hubel and Wiesel~\cite{Hubel1962}.

        Weights are no longer stored in a bidimensional matrix since all inputs are no longer connected to each neuron of the current layer.
        Instead, each neuron is connected to a certain neighborhood of inputs. In this way, the network drastically reduces its number of parameters
        but still takes the spatial dependence of the data into account. If the convolutional layer is designed for processing multi-channel images for example,
        the parameters will be stored in a 4-dimensional tensor. Let $W \in \mathbb{R}^{b \times h \times w \times n_c}$ be the weights of the convolutional filters,
        $X^{(p)} \in \mathbb{R}^{b \times h_b \times w_b \times n_c}$ the input images of layer $p$, $b \in \mathbb{R}$ the bias vector
        and $X^{(p+1)} \in \mathbb{R}^{b \times (\floor{(h_b - h) / \beta_1} + 1) \times (\floor{(w_b - w) / \beta_2} + 1) \times n_f}$  % \beta_1 and \beta_2 should defined here, not after the formula.
        the output feature maps. Let's consider the relation between the input images and the output feature maps:

        \begin{equation} \label{eq:conv2D}
            X_{i, j, k, l}^{(p+1)} = \sigma \Big( \sum\limits_{\alpha=1}^h \sum\limits_{\delta=1}^w
                \sum\limits_{c=1}^{n_c} W_{j, \alpha, \delta, c} X_{i, k+\beta_1 \alpha, l+\beta_2 \delta, c}^{(p)} + b_{j} \Big)
        \end{equation}

            where $i$ is the image identifier, $j$ is the filter index, $n_c$ is the number of channels, $h$ is the filter height, $w$ is the filter width
            and $(\beta_1, \beta_2)$ are the strides (vertical and horizontal distances between neighboring pixels in the neighborhood connected to a same neuron).
            Partial derivatives are simply given by:  % simply? Not sure it's necessary.

        \begin{align}  % /!\ \alpha is an index of the parameter for the partial derivative while also being an index of summation in the RHS.
            \frac{\partial X_{i, j, k, l}^{(p+1)}}{\partial W_{j, \alpha, \delta, c}} & =  % Maybe use commands \pd or \dpd from package commath for partial derivatives
                \sigma' \Big( \sum\limits_{\alpha=1}^h \sum\limits_{\delta=1}^w \sum\limits_{c=1}^{n_c} W_{j, \alpha, \delta, c}
                X_{i, k+\beta_1 \alpha, l+\beta_2 \delta, c}^{(p)} + b_{j} \Big) X_{i, k+\beta_1 \alpha, l+\beta_2 \delta, c} \\
            \frac{\partial Y_{i, j, k, l}}{\partial b_{j}} & =  % Why partial derivative of Y and not X?
                \sigma' \Big( \sum\limits_{\alpha=1}^h \sum\limits_{\delta=1}^w \sum\limits_{c=1}^{n_c} W_{j, \alpha, \delta, c}
                X_{i, k+\beta_1 \alpha, l+\beta_2 \delta, c}^{(p)} + b_{j} \Big)
        \end{align}

        Just as in the case of fully-connected layers, the computations for the signal propagation are not shown because this report is intended to remain brief.
        Contrary to neural networks, it must be noted that random forest implementations are rarely equipped with convolutional filters or even multivariate
        splits. Even in computer vision applications, univariate decision trees are the most frequently used trees in ensemble learners.

        \todo{Double check indices and whether symbols have been correctly defined}

    \subsection{Activation functions}

        An activation function describes the output value of a neuron and is biologically inspired. It is a mathematical representation
        of the level of action potential sent along its axon. More formally, it is a non-linear scalar function that takes a scalar as input.
        The presence of activation functions in neural networks along with fully-connected layers allows them to increase their representational
        power. Indeed, a stack of fully-connected layers without activation functions would have the same representational power as a single
        fully-connected layer, since a linear combination of linear combinations is itself a linear combination. Thus, activation functions
        help to actually build a hierarchical representation of the data by curving the hyperplane separating data points multiple times  % folding the hyperplane? In order to talk about hyperplanes, you need to be in an affine space and especially you require the projection to lie on a $n-1$ dimensional space which is highly likely not the case. Maybe just say "non-linearly modify the geometry of the projected space". But using "folding" formally would require to talk about manifolds.
        and at each layer.

        However, not every activation function is suitable for backpropagation and one of the reasons for the success of deep learning is the low
        computational requirements for the gradients. Most of the activation functions are non-parametric and element-wise, which makes it easy
        to compute the signal during backward pass.

        The best known activation function is the sigmoid function $\sigma(x)$.
        It has the property to have a derivative $\sigma'(x)$ expressed as a function of $\sigma(x)$,
        which speeds up computation times, assuming that the neural outputs are cached.

        \begin{equation}
            \begin{split}
                \sigma(x) & = \frac{1}{1 + \smallexp{-x}} = \frac{\smallexp{x}}{1 + \smallexp{x}} \\
                \sigma'(x) & = \sigma(x) (1 - \sigma(x))
            \end{split}
        \end{equation}

        However, LeCun~\cite{efficientBackprop} does not recommend standard sigmoid functions because normalizing
        activation functions generally ensure better performance.
        For this reason, the hyperbolic tangent is suitable because its outputs are centered around zero.
        Also, its derivative $\tanh'(x)$ is expressed as a function of $\tanh(x)$ which is computationally convenient.
        Finally, an additional linear term can be added in order to
        avoid flat areas, leading to an activation function of the following form: $f(x) = \tanh(x) + ax$.

        \begin{equation}
            \begin{split}
                \tanh(x) & = \frac{\smallexp{x} - \smallexp{-x}}{\smallexp{x} + \smallexp{-x}} = \frac{\smallexp{2x} - 1}{\smallexp{2x} + 1} \\
                \tanh'(x) & = 1 - \tanh^2(x)
            \end{split}
        \end{equation}

        Assuming that target values are in the set $\{-1, 1\}$ in the framework of binary classficaition,
        the hyperbolic tangent can be linearly modified to obtain a new function of the form $f(x) = 1.7159 \tanh(\frac{2}{3} x)$.
        Such an activation function is profitable because, has its second derivative maximized at $x = -1$ and $x = 1$, avoiding
        saturation effects.

        The chain rule informs us that the gradient of a given layer is factorized as a product of vectors/matrices computed by next layers.
        Because the absolute values of a layer's outputs are always less than one for both tanhand standard
        sigmoid activation functions, but also the absolute values of the gradient's components,
        deep architectures are often subject to vanishing gradients. Linear rectifier units (ReLU) are piecewise linear functions designed to solve
        these issues by keeping positive inputs unchanged.
        Let's note that ReLU is not differentiable at $x = 0$ but inputs can be reasonaly assumed to be rarely equal to zero in practice.

        \begin{equation}
            \begin{split}
                \text{ReLU}(x) & = \max{(x, 0)} \\
                \text{ReLU}'(x) & =
                \begin{cases}
                    1 & \text{if } x > 0 \\
                    0 & \text{if } x < 0
                \end{cases}
            \end{split}
        \end{equation}

        The outputs of a neural network are often desired to sum to one, especially when the classification task is to assign each class to a probability
        conditionally to the network's input. In the case where there are $m$ classes, the output layer is composed of $m$ neurons where the activation
        function associated to neuron $i$ is given by:

        \begin{equation}
            \begin{split}
                \sigma(x^{(i)}) & = \frac{\smallexp{x^{(i)}}}{\sum\limits_{k=1}^m \smallexp{x^{(k)}}} \\
                \sigma'(x^{(i)}) & = \sigma(x^{(i)}) (1 - \sigma(x^{(i)}))
            \end{split}
        \end{equation}

        where $x^{(i)}$ is the component $i$ of the output vector.  % Why write components as supscript and not subscript here?
        This function is identical to the Boltzmann distribution introduced in section \ref{potts}.


    \subsection{Batch normalization} \label{batchnorm}

        According to Ioffe and Szegedy~\cite{DBLP:journals/corr/IoffeS15}, deep neural networks are subject to a phenomenon
        called \textbf{internal covariate shift}. When the learning rate is too large, the distribution of a layer's output
        is drastically altered, making it difficult to train the next layer since the latter is constantly adapting to the new
        distribution. Batch normalization helps dealing with this issue and allows us to run the optimization algorithm with less
        careful parameter initialization and a larger learning rate.

        When the network is trained with batch learning, its parameters are updated at every batch. Therefore, the distribution
        of each layer's output is changed at each batch. This is the reason for using the statistics of each batch individually
        to normalize the data between layers.

        \todo{Backpropagating sample gradients in fully-convolution networks}

        \begin{equation}
            \begin{split}
                \mu_{\mathcal{B}} & = \frac{1}{\vert\mathcal{B}\vert} \sum\limits_{i=1}^{\vert\mathcal{B}\vert} x_i \\
                \sigma_{\mathcal{B}}^2 & = \frac{1}{\vert\mathcal{B}\vert} \sum\limits_{i=1}^{\vert\mathcal{B}\vert} (x_i - \mu_{\mathcal{B}}^2) \\
                \hat{x}_i & \leftarrow \gamma \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}} + \beta
            \end{split}
        \end{equation}

        Optimize $\beta, \gamma$ with backpropagation. \todo{}

    \subsection{Regularization}

    	From an optimization perspective, regularization is a penalty used to prevent
    	parameters from growing arbitrarily big during training.
    	According to Occam's law of parsimony, simpler hypotheses should be privileged over more complex ones.
    	Therefore, when the neural architecture involves a large number of free parameters
    	in the presence of relatively few data samples,
    	regularization helps reducing parameters importance and converging to less arbitrary parameter values.
    	From a Bayesian perspective, regularization provides a prior distribution over the model parameters.
    	In Bayes formula, the posterior $P(\theta \vert X, \alpha)$ is a function of both the prior
    	$P(\theta \vert \alpha)$ and the likelihood of the data $P(X \vert \theta, \alpha)$ under model $\theta$.

    	\begin{equation}
    	    P(\theta \vert X, \alpha) = \frac{P(X \vert \theta, \alpha)\,P(\theta \vert \alpha)}{P(X \vert \alpha)}
    	\end{equation}

    	The relation between the loss function of a neural network and Bayes formula can be established
    	by proving the two following points:

    	\begin{itemize}
    	    \item The log-likelihood of the data is equal to the negative cross-entropy.
    	    \item The regularization term is proportional to the prior distribution of the parameters.
    	\end{itemize}

    	The first part is easy to show since negative log-likelihood can be obtained from binary cross-entropy:
    	\begin{align}
    		CE(\hat{y}, y) & = - \log{\prod\limits_{i=0}^n P(\hat{y_i})^{y_i}}  \\
    		& = -\sum\limits_{i=0}^n y_i \log{\hat{y}} + (1 - y_i) \log{1 - \hat{y}}
    	\end{align}
    	This allows us to provide a statistical interpretation of the loss function.
    	Regarding priors, $L_1$ and $L_2$ regularizations are going to be introduced in the following two sections.

	\subsubsection{$L_1$ regularization}

	   Adding a $L_1$ regularization term to the loss function reduces to providing a Laplacian
	   prior on model parameters.

	   \begin{align}
	       \max_{\theta}\, \log{P(\theta \vert \eta, b)}
		   & = \max_{\theta}\, \log{\prod\limits_{i=1}^m \, \frac{1}{2b_i}\,
		   \exp{\frac{- \abs{\theta_i - \eta_i}}{b_i}}} \\
		   & = \max_{\theta}\, \sum\limits_{i=1}^m \frac{- \abs{\theta_i - \eta_i}}{b_i} - \log{2b_i} \\
		   & = \min_{\theta}\, \sum\limits_{i=1}^m \abs{\theta_i - \eta_i}
	   \end{align}

	   By setting vector $\eta \in \mathbb{R}^m$ to $0$, the resulting regularization term
	   takes its final well-known form $\sum\limits_{i=1}^m \abs{\theta_i}$.

	\subsubsection{$L_2$ regularization}

	   $L_2$ regularization acts as a Gaussian prior on model parameters.
	   This can be highlighted by setting the probability density function of the Gaussian
	   distribution as the prior and show that the regularization term is proportional
	   to the logarithm of the product of priors.

	   \begin{align}
               \max_{\theta}\, \log{P(\theta \vert \eta, \sigma)}
		   & = \max_{\theta}\, \log{\prod\limits_{i=1}^m \, \frac{1}{\sqrt{2\pi\sigma^2}}
		   \exp{-\frac{(\theta - \eta)^2}{2\sigma^2}}} \\
		   & = \max_{\theta}\, \sum\limits_{i=1}^m \, -\frac{(\theta_i - \eta_i)^2}{2\sigma^2}
		   - \log{\sqrt{2\pi\sigma^2}} \\
		   & = \min_{\theta}\, \sum\limits_{i=1}^m \, (\theta_i - \eta_i)^2
           \end{align}

	   Again, by setting vector $\eta \in \mathbb{R}^m$ to $0$, the regularization term takes its
	   final form $\sum\limits_{i=1}^m \theta_i^2$.

    \subsection{Optimization algorithms} \label{opti}

        Gradient descent is a very popular optimization algorithm,
        but is rarely used as such in practice since state-of-the-art deep learning
        frameworks offer more advanced gradient-based techniques~\cite{DBLP:journals/corr/Ruder16}.
        What is meant by gradient is the gradient vector obtained by concatenation of the gradients
        w.r.t. each layer's parameters. This final gradient vector
        gives an improvement direction, but a decrease of the loss function
        is only guaranteed by moving by an arbitrary small step in the parameter space.

        Gradient descent has three variants: batch, mini-batch and stochastic gradient descent.
        In batch gradient descent, all training examples are used to compute the improvement
        direction: this is done by computing the gradient for each training example and averaging
        across all examples. In mini-batch gradient descent, only a subset of training examples
        are being considered for the computation of the improvement direction
        (which can thus be seen as an approximation of the actual gradient). Usually, the ordering
        of training examples is shuffled at the beginning of each iteration (also called epoch)
        and then examples are sampled in the resulting order repeatedly,
        to ensure that each of them is seen by the model exactly once per iteration.
        In the stochastic variant, only the gradient of a single training example is used to
        approximate the improvement direction. Due to the high variability of gradients from one
        example to the other, the improvement direction is changing in a chaotic manner during
        the optimization process, hence the adjective stochastic.
        The three types of improvement vectors are summarized in table~\ref{tab:gradients}.

        \begin{table}[H]
            \centering
            \begin{tabular}{|l|c|c|}
                \hline
                Name & Number of examples involved & Formula \\
                \hline
                \hline
                Average (true) gradient & $N$ & $g_t = \frac{1}{N} \sum\limits_{i=1}^N \nabla L(f_{x_t}(Z_i))$ \\
                \hline
                Mini-batch gradient & $\vert B \vert$ & $g_t = \frac{1}{\vert B \vert} \sum\limits_{i \in B} \nabla L(f_{x_t}(Z_i))$ \\
                \hline
                Sample gradient & $1$ & $g_t = \nabla L(f_{x_t}(Z))$ \\
                \hline
            \end{tabular}
            \captionof{table}{Types of gradients and gradient approximations used in common optimization methods.
                Here the term "sample" refers to a sample of 1 example.}
            \label{tab:gradients}
        \end{table}

        In its most simplistic form, gradient descent optimization consists in update the parameter vector $x_t$
        from step $t$ using the following rule:
        \begin{align}
            x_{t+1} & = x_t + \Delta x_t \\
            & = x_t - \eta g_t
        \end{align}
        where update vector $\Delta x_t$ is equal to the negative approximated gradient $-g_t$ multiplied by
        a learning rate $\eta$. Learning rate controls how much model parameters are being updated
        in the improvement direction.

        Stochastic gradient descent has been extended with a so-called momentum term that accelerates
        the update when gradients approximately point in the same direction from one step to another.
        \todo{Momentum: cite: Learning representations by back-propagating errors}
        \begin{equation}
            \Delta x_t = \rho \Delta x_{t-1} - \eta g_t
        \end{equation}
        $\rho$ is a decay parameter that controls how much to keep track of past update vectors.
        In practice, the landscape of the loss function w.r.t. parameters is likely to be composed of many
        narrow valleys where standard stochastic gradient vector is inefficient due to the small norm of gradients
        along valleys. The momentum helps optimizing across such valleys in a smaller number of steps
        due to its additive effect when gradient vectors are similar from one step to the next one.

        \todo{Adam: \cite{DBLP:journals/corr/KingmaB14}}

        \todo{RMSProp: Introduced by Hinton on Coursera, first used in \cite{graves2013generating}}

        \todo{AdaGrad: \cite{duchi2011adaptive}}
        \begin{equation}
            \Delta x_t = - \frac{\eta}{\sum\limits_{\tau=1}^t g_{\tau}^2} g_t
        \end{equation}

        ADADELTA~\cite{DBLP:journals/corr/abs-1212-5701} is an adaptive extension of stochastic gradient descent
        that is robust to the noise introduced by the high variability of sample gradients. Also, it dynamically
        selects the learning rate so that no hyper-parameter tuning is required on it.
        \begin{equation}
            \Delta x_t = - \frac{RMS[\Delta x]_{t-1}}{RMS[g]_t} g_t
        \end{equation}
        $RMS[g]_t = \sqrt{E[g^2]_t + \epsilon}$

        \todo{L-BFGS: \cite{LBFGS}}


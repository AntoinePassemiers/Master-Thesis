\chapter{Introduction}

\section{Objectives of the thesis}

    \setcounter{page}{1}
    \vspace*{0.5cm}

    Proteins are large macromolecules in the form of chains of building blocks called amino acids,
    linked by peptide bonds. Because peptide bonds are formed through a dehydration reaction between
    the carboxyle group of an amino acid and the amino group of another amino acid,
    the resulting amino acids have no free group left (except the side chain) and are
    called residues. Because this thesis focuses on proteins and not amino acids alone,
    the terms "amino acid" and "residue" will be used interchangeably.

    There are 20 common amino acid types, but certain proteins may contain 2
    additional amino acid types, namely pyrrolysine and selenocystein.
    Proteins are responsible for a wide range of functions within living organisms, including
    enzyme catalysis, transport of molecules, DNA replication, DNA repair, DNA transcription or cell signaling.
    Over 5000 types of biochemical reactions have been shown as being catalyzed by enzymes~\cite{schomburg2012brenda},
    which are mostly proteins. The number a ligands a protein can bind, namely the enzymatic specificity,
    can be determined by the structure of the protein itself~\cite{pi2004determination}.
    The protein region binding a substrate and containing
    the residues involved in the catalytic process is called the active site.
    Enzyme structure is thus of great importance since enzymatic specificity is crucial in novel drug discovery:
    molecules present in tested drugs are expected to have a structure with as large as possible specificity
    in order to avoid unwanted effects on the patient.

    According to Anfinsen's dogma~\cite{anfinsen1973principles}, the structure of a protein is uniquely
    determined by its underlying amino acid sequence,
    at least when observed in protein's native environment.
    When moved from an unfavourable environment (where proper folding conditions are not met)
    to a solvent at neutral pH, a random coil (a sequence of amino acid residues
    oriented in random directions) will evolve towards the three-dimensional structure that minimizes Gibbs free energy.
    This process is called protein folding and has, however, a few exceptions.

    Protein structure is organized hierarchically: primary structure, secondary structure, tertiary structure
    and quaternary structure. Primary structure refers to the chemical composition of the protein, hence the sequence of amino acids present in it.
    Secondary structure relates to the presence of structures that are local to the amino acids themselves:
    these structures are mostly $\alpha$-helices
    and $\beta$-sheets, but multiple other structural classes exist.
    Tertiary structure contains information about the three-dimensional
    structure of the protein and results from interactions
    between side chains of some pairs of amino acids, such as hydrogen bonds, ionic bonds or disulfide bridges.
    Quaternary structure is specific to proteins having multiple polypeptide
    chains and relates to the structure due to intermolecular interactions between
    these chains. Protein contact prediction (PCP) helps predicting the tertiary structure as three-dimensional
    models can be reconstructed from protein contact maps (see section \ref{contactfold}).
    Protein contact maps are a more simplistic and robust description of a protein's geometry
    because they are invariant to rotations and translations.
    This simplification helps making deep learning methods perform well on structure prediction.

    Assuming the protein backbone has no structural restriction and is composed of $n$ residues holded together by $n-1$ peptide bonds,
    then the protein has $2(n-1)$ bond angles that can be each in three different stable states. Therefore, there are at most
    $3^{2(n-1)}$ possible configurations, and it would take the age of the universe to find the correct folding by enumeration.
    There is strong evidence that protein folding is a NP-hard problem~\cite{hart1997robust}.
    However, in practice small proteins are able to fold into a stable conformation in a fraction of a millisecond.
    This observation is known as the Levinthal's paradox. There has been a long standing perspective that protein folding
    is guided by heuristics composed of local interactions~\cite{levinthal1969fold}. Heuristic folding leads to misfolded proteins
    that can potentially cause genetic diseases.
    Luckily, some proteins are assisted by molecular chaperones during their folding process~\cite{ellis1991molecular}
    to attain their functional conformation. It must be noted that Anfinsen's observations of polypeptide chains refolding
    spontaneously in an aqueous medium have been made in the framework of in vitro studies:
    they do not take into account protein-protein interations and thus cannot generalize the self-assembly process well.

    Computational methods are important in structural biology,
    as they help in assigning biochemical or biological functions to proteins
    in an autonomated manner.
    The three-dimensional structure of a protein is more conserved than the
    underlying amino acid sequence across evolution, partly due to the fact that
    mutations between amino acids having the same physico-chemical properties are frequent.
    Prompted by this knowledge, similar functions
    can be assigned to proteins with low structural dissimilarity.
    Precisely identifying the role played by each protein in an organism is the first step
    towards understanding complex body mechanisms like muscle contraction, digestion or perceiving light.
    Also, determining the static structure of proteins help in detecting misfolded proteins
    which are possibly involved in diseases like Parkinson's or
    Alzheimer's, but also in diagnosing those diseases~\cite{forloni2002protein}.
    Finally, solving the protein folding problem will enable
    better protein design, for example to engineer enzymes like PETase
    so they have faster plastic-degrading capabilities~\cite{DeepMind}.

    Protein Contact Prediction can help determining the three-dimensional structure of
    proteins by limiting the search space to certain conformations that are
    constrained by the predicted contact maps: this methodology is called contact-assisted protein folding.
    The problem of predicting the structure of a protein can start by a PCP stage because the latter is a much simpler problem
    (despite in itself being very hard), and only a few correctly predicted contacts
    are sufficient to reconstruct the whole structure~\cite{kim2014one}.
    There are multiple well-established pipelines for the structural prediction of a newly
    observed protein, such as RaptorX server~\cite{peng2011raptorx}.

    Most state-of-the-art PCP methods can be roughly divided into two categories:
    the ones based on Evolutionary Coupling Analysis (ECA) and the ones that infer contacts using
    supervised machine learning. In the former case, amino acid pairwise mutations
    are statistically modelled and the underlying model's parameters
    are optimized through log-likelihood maximization or the optimization of any similar metric.
    In the second case, deep neural architectures are used to
    refine predictions made by low-level predictors such as ECA, in order to generate high-quality contact maps.

    Ultimately, PCP should help making \textit{ab initio} structure prediction in a single pass.
    However, most recent methods rely on a whole raft of database search, alignment, prediction and folding tools.
    Given a protein encoded in FASTA format, ECA is only possible using a Multiple Sequence Alignment (MSA)
    of this target protein against homologuous proteins.
    These homologous proteins usually come from the same protein family
    as the target protein.
    This can be done by matching the target sequence to a Hidden Markov Model (HMM) profile representing a family
    like in Pfam database~\cite{Pfam}. Once the homologuous sequences have been retrieved, they have to be aligned to
    the target sequence using an MSA tool like HHblits or HMMER. In the next step, evolutionary couplings are extracted from
    the MSA using an ECA predictor like PSICOV~\cite{doi:10.1093/bioinformatics/btr638} or plmDCA~\cite{EKEBERG2014341}.
    Eventually, predictions are gathered and refined using a deep neural architecture, necessitating the use
    of a deep learning framework. These successive layers of dependencies are not making PCP a straightforward process.
    Therefore, it seems to be a natural choice to set as an objective for this thesis the development of a predictor with
    minimal requirements and performance close to state-of-the-art techniques.

\section{Contributions}

    During the writing of this thesis, I've been confronted with the need to
    adopt a full workflow for data retrieval and pre-processing and to develop a supervised
    model for accurate protein contact prediction. In order to be able to compete
    with state-of-the-art models, I had recourse to deep residual
    neural networks~\cite{DBLP:journals/corr/HeZRS15} and implemented them in a
    fully-convolutional manner.
    Indeed, the local context of a residue pair can be captured by stacking many
    convolutional layers and therefore increasing the receptive field~\cite{Hubel1962}
    of the output layers of the network.
    The number of neighbouring input values made visible to a same hidden neuron,
    or receptive field, increases linearly with the depth
    of the neural network when using standard convolutional filters.
    Therefore, in order to reach the context size required to capture the long-range information
    of large proteins, a large number of layers had to be considered.
    With the aim of overcoming common issues
    encountered when growing very large architectures,
    residual connections~\cite{DBLP:journals/corr/HeZRS15}
    (legitimating the use of a ResNet) as well as batch
    normalization~\cite{DBLP:journals/corr/IoffeS15} have been introduced.
    Batch normalization helps preventing internal covariate shift,
    a shift in the distribution of a layer's output due to the update of the parameters,
    which causes the inability of the next layers to learn efficiently.

    Finally, in order to promote and facilitate academic research on the topic, I have in all modesty
    open-sourced all the work I've done during the year of thesis writing.
    Due to my computer science background, it is my belief that the
    available code could serve as example for biologists and bioinformaticians
    with lower capabilities in programming.

\section{Structure of the thesis}

    Let's describe the global view of the thesis itself.
    Firstly, common state-of-the-art ECA techniques will be described, such as Direct Coupling Analysis (DCA)
    and Pseudo-Inverse Covariance matrices (PSICOV) (both are statistical methods based on graphical models),
    as well as the backpropagation algorithm and deep learning concepts involved in the design
    of the model developped during the thesis.
    In order to gain a deeper insight on best-performing methods, more details will be given for
    some specific deep learning methods, such as their architecture, input features and preprocessing.
    The generic architecture in use for this thesis will be detailed, as well as the hyper-parameter
    optimization procedure used for cross-validation.
    Finally, results will be presented in multiple sections:
    \begin{itemize}
        \item Since the proposed deep learning approach relies on DCA predictions as input features,
        the first step should demonstrate that deep learning is capable of refining
        contact maps by looking at complex visual patterns that cannot captured by linear models.
        \item It should be brought to light whether deep learning's performance is less sensitive to the effective
        number of homologous sequences than DCA methods. The notion of effective number of homologous sequences
        will be introduced in section \ref{meff}.
        \item Finally, a benchmark will be established to assess the performance of the proposed method
        in comparison with other supervised approaches, including state-of-the-art deep learning architectures.
    \end{itemize}
